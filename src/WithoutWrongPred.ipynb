{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n",
      "Data package template written to: /app/_data/models/final/without_wrong_pred/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets init -p /app/_data/models/final/without_wrong_pred/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n",
      "Starting upload for file eff4_0858_without_wrong_pred_3.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:16<00:00, 674kB/s]\n",
      "Upload successful: eff4_0858_without_wrong_pred_3.h5 (203MB)\n",
      "Starting upload for file eff4_0858_without_wrong_pred_5.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:19<00:00, 667kB/s]\n",
      "Upload successful: eff4_0858_without_wrong_pred_5.h5 (203MB)\n",
      "Starting upload for file eff4_0858_without_wrong_pred_1.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:18<00:00, 668kB/s]\n",
      "Upload successful: eff4_0858_without_wrong_pred_1.h5 (203MB)\n",
      "Starting upload for file eff4_0858_without_wrong_pred_4.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:14<00:00, 677kB/s]\n",
      "Upload successful: eff4_0858_without_wrong_pred_4.h5 (203MB)\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file eff4_0858_without_wrong_pred_2.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:24<00:00, 657kB/s]\n",
      "Upload successful: eff4_0858_without_wrong_pred_2.h5 (203MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/nataliayurasova/WithoutWrongPred\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets create -p /app/_data/models/final/without_wrong_pred/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.891\n",
    "MODEL_BB_PATH= '../input/model-bb-1/bond_box_999_200.h5'\n",
    "MODEL_PATH = '../input/0865fulltrain/'\n",
    "IMAGE_SIZE = (380, 380)\n",
    "DF_PART = '../input/df-kf-plant/df_kf.csv'\n",
    "PATH = \"/kaggle/input/plant-pathology-2021-fgvc8/\"\n",
    "TRAIN_IMG_PATH = PATH+'train_images/'\n",
    "TEST_IMG_PATH = PATH+'test_images/'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES=6\n",
    "SEED = 1488\n",
    "- replace ''-'scab'\n",
    "https://www.kaggle.com/nataliayurasova/plant-pathology0891/edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (0.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.17.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (8.1.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.8/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install albumentations\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    train_test_split,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, EfficientNetB7\n",
    "from tensorflow.keras.layers import (\n",
    "    AveragePooling2D,\n",
    "    AvgPool2D,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import notebook, tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/app/_data/\"\n",
    "BATCH_SIZE = 54\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 380\n",
    "NUM_CLASSES = 6\n",
    "TRAIN_IMG_PATH = \"/app/_data/380_npy/\"\n",
    "TEST_IMG_PATH = \"/app/_data/test_images/\"\n",
    "feature_columns = [\n",
    "    \"complex\",\n",
    "    \"frog_eye_leaf_spot\",\n",
    "    \"healthy\",\n",
    "    \"powdery_mildew\",\n",
    "    \"rust\",\n",
    "    \"scab\",\n",
    "]\n",
    "wrong = ['ead085dfac287263.jpg', '95276ccd226ad933.jpg',\"da8770e819d2696d.jpg\", 'cd3a1d64e6806eb5.jpg', 'ccec54723ff91860.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(\"../_data/df_csv/labels_21_20.csv\", index_col=[0])\n",
    "df_labels = df_labels.query('image not in @wrong').reset_index(drop=True)\n",
    "wrong_eff4_ns = pd.read_csv('/app/sandbox/wrong_predictions/eff4/wrong_eff4_ns.csv',  index_col=[0])\n",
    "list_drop = wrong_eff4_ns[\"image\"].tolist()\n",
    "df_labels = df_labels.query('image not in @list_drop')\n",
    "df_labels[\"image\"] = df_labels[\"image\"].str.replace(\".jpg\", \".npy\")\n",
    "wrong_eff4_ids = pd.read_csv('/app/sandbox/wrong_predictions/eff4/wrong_eff4_ids.csv',  index_col=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_eff4_ids = wrong_eff4_ids.join(wrong_eff4_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2012, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2056, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_eff4_ns.shape\n",
    "wrong_eff4_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complex</th>\n",
       "      <th>complex_true</th>\n",
       "      <th>frog_eye_leaf_spot</th>\n",
       "      <th>frog_eye_leaf_spot_true</th>\n",
       "      <th>healthy</th>\n",
       "      <th>healthy_true</th>\n",
       "      <th>powdery_mildew</th>\n",
       "      <th>powdery_mildew_true</th>\n",
       "      <th>rust</th>\n",
       "      <th>rust_true</th>\n",
       "      <th>scab</th>\n",
       "      <th>scab_true</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>daa3559ae348da45.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>fc9692c95bc8c4aa.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>9eb1f0c984e52dc6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>d1d288f558ad6d86.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>af72d2854aae750c.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2056 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      complex  complex_true  frog_eye_leaf_spot  frog_eye_leaf_spot_true  \\\n",
       "0          12            13                   5                       13   \n",
       "1          11            13                   5                       13   \n",
       "2           1             0                  13                       13   \n",
       "3           0             0                   9                        0   \n",
       "4           1             0                  11                        0   \n",
       "...       ...           ...                 ...                      ...   \n",
       "2051       12            13                   8                       13   \n",
       "2052        5             0                   7                       13   \n",
       "2053        7             9                   9                        0   \n",
       "2054        4            13                   0                        0   \n",
       "2055        1             0                   7                       13   \n",
       "\n",
       "      healthy  healthy_true  powdery_mildew  powdery_mildew_true  rust  \\\n",
       "0           0             0               0                    0     0   \n",
       "1           0             0               0                    0     0   \n",
       "2           0             0               0                    0     0   \n",
       "3           0             0               0                    0     0   \n",
       "4           0             0               0                    0     0   \n",
       "...       ...           ...             ...                  ...   ...   \n",
       "2051        0             0               0                    0     0   \n",
       "2052        0             0               0                    0     6   \n",
       "2053        0             0               0                    0     0   \n",
       "2054        0             0              13                   13     0   \n",
       "2055        0             0               0                    0     0   \n",
       "\n",
       "      rust_true  scab  scab_true                 image  \n",
       "0             0     6         13  daa3559ae348da45.jpg  \n",
       "1             0     5         13  fc9692c95bc8c4aa.jpg  \n",
       "2             0     4         13  9eb1f0c984e52dc6.jpg  \n",
       "3             0     8          9  d1d288f558ad6d86.jpg  \n",
       "4             0     9         11  af72d2854aae750c.jpg  \n",
       "...         ...   ...        ...                   ...  \n",
       "2051          0     5         13                   NaN  \n",
       "2052         13     0          0                   NaN  \n",
       "2053          0     1          0                   NaN  \n",
       "2054          0     0          0                   NaN  \n",
       "2055          0    13         13                   NaN  \n",
       "\n",
       "[2056 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_eff4_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 15))\n",
    "# for i, img_name in enumerate(wrong_eff4_ns.sample(20)['image'].tolist()):\n",
    "#     plt.subplot(5,4,i+1)\n",
    "#     label = df_labels[df_labels[\"image\"]==img_name]['labels'].values\n",
    "#     img = np.load(TRAIN_IMG_PATH+img_name)\n",
    "#     plt.title(label)\n",
    "#     plt.imshow(img)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df_labels.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18213, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_to_drop = []\n",
    "# for ind in df_labels['image'].tolist():\n",
    "#     if ind not in os.listdir('/app/_data/380_npy/'):\n",
    "#         images_to_drop.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_to_drop = ['gi_powdery_mildew_10036.jpeg',\n",
    "#  'gi_powdery_mildew_10122.jpeg',\n",
    "#  'gi_powdery_mildew_10017.jpeg',\n",
    "#  'gi_rust_disease_leaves_10057.jpeg',\n",
    "#  'gi_rust_disease_leaves_10100.jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labels = df_labels.query('image not in @images_to_drop').reset_index(drop=True)\n",
    "# df_labels = df_labels.join(df_labels['labels'].str.get_dummies(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image                 0\n",
       "labels                0\n",
       "complex               0\n",
       "frog_eye_leaf_spot    0\n",
       "healthy               0\n",
       "powdery_mildew        0\n",
       "rust                  0\n",
       "scab                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = labels_21_20.copy()\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.07, random_state=SEED)\n",
    "\n",
    "# for train_index, valid_index in sss.split(df[\"image\"], df[\"labels\"]):\n",
    "#     train, valid = df.loc[train_index], df.loc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 380*380\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.CLAHE(p=0.1, clip_limit=(1, 2), tile_grid_size=(8, 8)),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.MotionBlur((3, 3)),\n",
    "                albumentations.MedianBlur(blur_limit=3),\n",
    "                albumentations.GaussianBlur(blur_limit=(3, 3), sigma_limit=0),\n",
    "                albumentations.Blur(blur_limit=(3, 3)),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.GaussNoise(var_limit=[10, 50], mean=1),\n",
    "                albumentations.ISONoise(intensity=(0.1, 1), color_shift=(0.01, 0.05)),\n",
    "                albumentations.ImageCompression(\n",
    "                    quality_lower=70, quality_upper=100, compression_type=1\n",
    "                ),\n",
    "                albumentations.MultiplicativeNoise(\n",
    "                    multiplier=(0.95, 1.05), per_channel=True, elementwise=True\n",
    "                ),\n",
    "                albumentations.Downscale(\n",
    "                    scale_min=0.6, scale_max=0.99, interpolation=4\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.HueSaturationValue(\n",
    "                    hue_shift_limit=(-7, 7),\n",
    "                    sat_shift_limit=(-10, 10),\n",
    "                    val_shift_limit=(-10, 10),\n",
    "                ),\n",
    "                albumentations.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.15,\n",
    "                    contrast_limit=0.2,\n",
    "                    brightness_by_max=True,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(\n",
    "                    distort_limit=0.05,\n",
    "                    shift_limit=0.05,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "                albumentations.ElasticTransform(\n",
    "                    alpha=2.0,\n",
    "                    sigma=50.0,\n",
    "                    alpha_affine=10.0,\n",
    "                    interpolation=0,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "                albumentations.GridDistortion(\n",
    "                    num_steps=5, distort_limit=0.3, interpolation=0, border_mode=2\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.HorizontalFlip(),\n",
    "                albumentations.VerticalFlip(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.Rotate(\n",
    "                    limit=(-180, 180), interpolation=0, border_mode=2\n",
    "                ),\n",
    "                albumentations.ShiftScaleRotate(\n",
    "                    shift_limit=0.05,\n",
    "                    scale_limit=0.05,\n",
    "                    rotate_limit=180,\n",
    "                    interpolation=0,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        images_src_dir,\n",
    "        batch_size,\n",
    "        target_image_size,\n",
    "        shuffle=False,\n",
    "        augment=True,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "        normalize=False,\n",
    "    ):\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.df = df\n",
    "        self.images_dir = images_src_dir\n",
    "        self.target_image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "        self.augment = augment\n",
    "        self.crop = crop\n",
    "        self.resize = resize\n",
    "        self.normalize = normalize\n",
    "        # create label index map\n",
    "        self.labels = self._read_labels()\n",
    "        self.n_samples = self.df.shape[0]\n",
    "        self.n_batches = self.n_samples // self.batch_size\n",
    "        # shuffle data, also repeated after each epoch if needed\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.labels)\n",
    "\n",
    "    def _read_labels(self):\n",
    "        \"\"\"\n",
    "        Returns list images mapping to 1-hot label\n",
    "        \"\"\"\n",
    "\n",
    "        # label indexes\n",
    "        label_ixs = self.df[feature_columns].values\n",
    "        image_ixs = self.df[\"image\"].values\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(image_ixs)):\n",
    "            labels.append([image_ixs[i], label_ixs[i]])\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length in batches\n",
    "        \"\"\"\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, b_ix):\n",
    "        \"\"\"\n",
    "        Produce batch, by batch index\n",
    "        \"\"\"\n",
    "\n",
    "        assert b_ix < self.n_batches\n",
    "\n",
    "        b_X = np.zeros(\n",
    "            (self.batch_size, self.target_image_size[0], self.target_image_size[1], 3),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        b_Y = np.zeros(\n",
    "            (self.batch_size, self.df[feature_columns].shape[1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            b_X[i], b_Y[i] = self.get_one(\n",
    "                i + self.batch_size * b_ix,\n",
    "            )\n",
    "\n",
    "        return (b_X, b_Y)\n",
    "\n",
    "    def get_one(self, one_ix):\n",
    "        \"\"\"\n",
    "        Get single item by absolute index\n",
    "        \"\"\"\n",
    "        id = self.labels[one_ix][0]\n",
    "        src_file = self.images_dir + id\n",
    "\n",
    "        # read file\n",
    "        x = np.load(src_file)\n",
    "        if self.crop:\n",
    "            coord = self.df[self.df[\"image\"] == id][\n",
    "                [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]\n",
    "            ].values[0]\n",
    "            orig_hight = x.shape[0]\n",
    "            orig_width = x.shape[1]\n",
    "            x_min = coord[0]\n",
    "            y_min = coord[1]\n",
    "            x_max = coord[2]\n",
    "            y_max = coord[3]\n",
    "            x = x[\n",
    "                np.int(y_min * orig_hight) : np.int(y_max * orig_hight),\n",
    "                np.int(x_min * orig_width) : np.int(x_max * orig_width),\n",
    "            ]\n",
    "\n",
    "        y = self.labels[one_ix][1]\n",
    "\n",
    "        # augment\n",
    "        if self.augment:\n",
    "            x = self._augment_image(x)\n",
    "\n",
    "        # normalize (sample-wise)\n",
    "        if self.normalize:\n",
    "            x = x.astype(np.float32)\n",
    "            x = x - np.mean(x, axis=(0, 1))\n",
    "            x = x / np.std(x, axis=(0, 1))\n",
    "        return x.astype(np.uint8), y\n",
    "\n",
    "    def _augment_image(self, x):\n",
    "        \"\"\"\n",
    "        Randomply augment image\n",
    "        \"\"\"\n",
    "\n",
    "        x = transform(image=x)[\"image\"]\n",
    "        return x\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "policy = keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    base_model = keras.applications.EfficientNetB4(weights=None, include_top=False)\n",
    "    base_model.load_weights(\n",
    "        \"/app/_data/models/efficientnet-b4_noisy-student_notop.h5\",\n",
    "        by_name=True,\n",
    "        skip_mismatch=True,\n",
    "    )\n",
    "    x = base_model(inputs)\n",
    "    x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "    x = keras.layers.Flatten(name=\"flatten\")(x)\n",
    "    outputs = keras.layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(lr=0.0005),\n",
    "        metrics=[\n",
    "            \"acc\",\n",
    "            tfa.metrics.F1Score(num_classes=NUM_CLASSES, average=\"weighted\"),\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_logger(self, logs: dict):\n",
    "#     \"\"\" Sample logger - adds LR \"\"\"\n",
    "#     logs.update({\"lr\": keras.backend.eval(self.model.optimizer.lr)})\n",
    "\n",
    "\n",
    "# def gpu_temp_logger(self, logs: dict):\n",
    "#     stream = os.popen(\"nvidia-smi -i 0 --query-gpu='temperature.gpu' --format=csv,noheader\")\n",
    "#     output = stream.read()\n",
    "#     gpu_temp = int(output)\n",
    "#     logs.update({\"gpu_temp\": gpu_temp})\n",
    "\n",
    "\n",
    "# class TensorBoard_Logger(keras.callbacks.TensorBoard):\n",
    "#     def __init__(self, loggers=[lr_logger], **kwargs):\n",
    "#         self._loggers = loggers\n",
    "#         super().__init__(**kwargs)\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         for logger in self._loggers:\n",
    "#             logger(self, logs)\n",
    "#         super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 0 skf\n",
      "\n",
      "Epoch 13/100\n",
      "  1/269 [..............................] - ETA: 0s - loss: 0.0054 - acc: 1.0000 - f1_score: 1.0000WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0208 - acc: 0.9800 - f1_score: 0.9799\n",
      "Epoch 00013: val_f1_score improved from -inf to 0.97748, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_1.h5\n",
      "269/269 [==============================] - 119s 442ms/step - loss: 0.0208 - acc: 0.9800 - f1_score: 0.9799 - val_loss: 0.0243 - val_acc: 0.9776 - val_f1_score: 0.9775\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9816 - f1_score: 0.9815\n",
      "Epoch 00014: val_f1_score improved from 0.97748 to 0.98178, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_1.h5\n",
      "269/269 [==============================] - 108s 401ms/step - loss: 0.0175 - acc: 0.9816 - f1_score: 0.9815 - val_loss: 0.0188 - val_acc: 0.9818 - val_f1_score: 0.9818\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0154 - acc: 0.9844 - f1_score: 0.9844\n",
      "Epoch 00015: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0154 - acc: 0.9844 - f1_score: 0.9844 - val_loss: 0.0379 - val_acc: 0.9729 - val_f1_score: 0.9731\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0147 - acc: 0.9851 - f1_score: 0.9851\n",
      "Epoch 00016: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 114s 424ms/step - loss: 0.0147 - acc: 0.9851 - f1_score: 0.9851 - val_loss: 0.0198 - val_acc: 0.9795 - val_f1_score: 0.9796\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0144 - acc: 0.9852 - f1_score: 0.9852\n",
      "Epoch 00017: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0144 - acc: 0.9852 - f1_score: 0.9852 - val_loss: 0.0300 - val_acc: 0.9765 - val_f1_score: 0.9766\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9865 - f1_score: 0.9865\n",
      "Epoch 00018: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0134 - acc: 0.9865 - f1_score: 0.9865 - val_loss: 0.0310 - val_acc: 0.9710 - val_f1_score: 0.9707\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0153 - acc: 0.9840 - f1_score: 0.9839\n",
      "Epoch 00019: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 116s 433ms/step - loss: 0.0153 - acc: 0.9840 - f1_score: 0.9839 - val_loss: 0.0244 - val_acc: 0.9762 - val_f1_score: 0.9763\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.9891 - f1_score: 0.9891\n",
      "Epoch 00020: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0110 - acc: 0.9891 - f1_score: 0.9891 - val_loss: 0.0226 - val_acc: 0.9793 - val_f1_score: 0.9794\n",
      "Epoch 21/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0135 - acc: 0.9864 - f1_score: 0.9864\n",
      "Epoch 00021: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 114s 425ms/step - loss: 0.0135 - acc: 0.9864 - f1_score: 0.9864 - val_loss: 0.0295 - val_acc: 0.9765 - val_f1_score: 0.9763\n",
      "Epoch 22/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0135 - acc: 0.9855 - f1_score: 0.9855\n",
      "Epoch 00022: val_f1_score did not improve from 0.98178\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0135 - acc: 0.9855 - f1_score: 0.9855 - val_loss: 0.0305 - val_acc: 0.9726 - val_f1_score: 0.9722\n",
      "Epoch 23/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0123 - acc: 0.9879 - f1_score: 0.9879\n",
      "Epoch 00023: val_f1_score did not improve from 0.98178\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0123 - acc: 0.9879 - f1_score: 0.9879 - val_loss: 0.0213 - val_acc: 0.9782 - val_f1_score: 0.9780\n",
      "Epoch 24/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9904 - f1_score: 0.9904\n",
      "Epoch 00024: val_f1_score improved from 0.98178 to 0.98423, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_1.h5\n",
      "269/269 [==============================] - 118s 438ms/step - loss: 0.0095 - acc: 0.9904 - f1_score: 0.9904 - val_loss: 0.0206 - val_acc: 0.9842 - val_f1_score: 0.9842\n",
      "Epoch 25/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9888 - f1_score: 0.9888\n",
      "Epoch 00025: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0104 - acc: 0.9888 - f1_score: 0.9888 - val_loss: 0.0263 - val_acc: 0.9804 - val_f1_score: 0.9803\n",
      "Epoch 26/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9892 - f1_score: 0.9892\n",
      "Epoch 00026: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0101 - acc: 0.9892 - f1_score: 0.9892 - val_loss: 0.0327 - val_acc: 0.9732 - val_f1_score: 0.9732\n",
      "Epoch 27/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9902 - f1_score: 0.9902\n",
      "Epoch 00027: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0101 - acc: 0.9902 - f1_score: 0.9902 - val_loss: 0.0249 - val_acc: 0.9790 - val_f1_score: 0.9790\n",
      "Epoch 28/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9906 - f1_score: 0.9906\n",
      "Epoch 00028: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0095 - acc: 0.9906 - f1_score: 0.9906 - val_loss: 0.0190 - val_acc: 0.9815 - val_f1_score: 0.9815\n",
      "Epoch 29/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9911 - f1_score: 0.9911\n",
      "Epoch 00029: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 114s 423ms/step - loss: 0.0086 - acc: 0.9911 - f1_score: 0.9911 - val_loss: 0.0234 - val_acc: 0.9771 - val_f1_score: 0.9771\n",
      "Epoch 30/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0099 - acc: 0.9897 - f1_score: 0.9897\n",
      "Epoch 00030: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0099 - acc: 0.9897 - f1_score: 0.9897 - val_loss: 0.0815 - val_acc: 0.9502 - val_f1_score: 0.9488\n",
      "Epoch 31/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0089 - acc: 0.9916 - f1_score: 0.9916\n",
      "Epoch 00031: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0089 - acc: 0.9916 - f1_score: 0.9916 - val_loss: 0.0201 - val_acc: 0.9807 - val_f1_score: 0.9807\n",
      "Epoch 32/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0087 - acc: 0.9912 - f1_score: 0.9912\n",
      "Epoch 00032: val_f1_score did not improve from 0.98423\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0004050000192364678.\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0087 - acc: 0.9912 - f1_score: 0.9912 - val_loss: 0.0242 - val_acc: 0.9812 - val_f1_score: 0.9812\n",
      "Epoch 33/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9925 - f1_score: 0.9925\n",
      "Epoch 00033: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0076 - acc: 0.9925 - f1_score: 0.9925 - val_loss: 0.0215 - val_acc: 0.9829 - val_f1_score: 0.9828\n",
      "Epoch 34/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9917 - f1_score: 0.9917\n",
      "Epoch 00034: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0072 - acc: 0.9917 - f1_score: 0.9917 - val_loss: 0.0270 - val_acc: 0.9804 - val_f1_score: 0.9801\n",
      "Epoch 35/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9917 - f1_score: 0.9917Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_f1_score did not improve from 0.98423\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0083 - acc: 0.9917 - f1_score: 0.9917 - val_loss: 0.0277 - val_acc: 0.9754 - val_f1_score: 0.9752\n",
      "Epoch 00035: early stopping\n",
      "\n",
      "Starting 1 skf\n",
      "\n",
      "Epoch 1/100\n",
      "  2/269 [..............................] - ETA: 6:19 - loss: 0.6758 - acc: 0.1204 - f1_score: 0.1155WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.5868s vs `on_train_batch_end` time: 2.2592s). Check your callbacks.\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0997 - acc: 0.8925 - f1_score: 0.8912\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.90431, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_2.h5\n",
      "269/269 [==============================] - 124s 461ms/step - loss: 0.0997 - acc: 0.8925 - f1_score: 0.8912 - val_loss: 0.0910 - val_acc: 0.9074 - val_f1_score: 0.9043\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0470 - acc: 0.9524 - f1_score: 0.9521\n",
      "Epoch 00002: val_f1_score improved from 0.90431 to 0.95181, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_2.h5\n",
      "269/269 [==============================] - 117s 433ms/step - loss: 0.0470 - acc: 0.9524 - f1_score: 0.9521 - val_loss: 0.0482 - val_acc: 0.9558 - val_f1_score: 0.9518\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0369 - acc: 0.9612 - f1_score: 0.9610\n",
      "Epoch 00003: val_f1_score improved from 0.95181 to 0.97142, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_2.h5\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0369 - acc: 0.9612 - f1_score: 0.9610 - val_loss: 0.0275 - val_acc: 0.9721 - val_f1_score: 0.9714\n",
      "Epoch 4/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9650 - f1_score: 0.9648\n",
      "Epoch 00004: val_f1_score did not improve from 0.97142\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0329 - acc: 0.9650 - f1_score: 0.9648 - val_loss: nan - val_acc: 0.9632 - val_f1_score: 0.9635\n",
      "Epoch 5/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0285 - acc: 0.9712 - f1_score: 0.9711\n",
      "Epoch 00005: val_f1_score did not improve from 0.97142\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0285 - acc: 0.9712 - f1_score: 0.9711 - val_loss: 0.0308 - val_acc: 0.9713 - val_f1_score: 0.9702\n",
      "Epoch 6/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9723 - f1_score: 0.9722\n",
      "Epoch 00006: val_f1_score did not improve from 0.97142\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0277 - acc: 0.9723 - f1_score: 0.9722 - val_loss: 0.0356 - val_acc: 0.9663 - val_f1_score: 0.9645\n",
      "Epoch 7/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0235 - acc: 0.9756 - f1_score: 0.9755\n",
      "Epoch 00007: val_f1_score improved from 0.97142 to 0.97593, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_2.h5\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0235 - acc: 0.9756 - f1_score: 0.9755 - val_loss: 0.0256 - val_acc: 0.9760 - val_f1_score: 0.9759\n",
      "Epoch 8/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0219 - acc: 0.9775 - f1_score: 0.9774\n",
      "Epoch 00008: val_f1_score did not improve from 0.97593\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0219 - acc: 0.9775 - f1_score: 0.9774 - val_loss: 0.0307 - val_acc: 0.9696 - val_f1_score: 0.9687\n",
      "Epoch 9/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.9773 - f1_score: 0.9773\n",
      "Epoch 00009: val_f1_score improved from 0.97593 to 0.98112, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_2.h5\n",
      "269/269 [==============================] - 117s 433ms/step - loss: 0.0224 - acc: 0.9773 - f1_score: 0.9773 - val_loss: 0.0190 - val_acc: 0.9812 - val_f1_score: 0.9811\n",
      "Epoch 10/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9793 - f1_score: 0.9793\n",
      "Epoch 00010: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 117s 436ms/step - loss: 0.0204 - acc: 0.9793 - f1_score: 0.9793 - val_loss: 0.0200 - val_acc: 0.9801 - val_f1_score: 0.9801\n",
      "Epoch 11/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9818 - f1_score: 0.9817\n",
      "Epoch 00011: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0172 - acc: 0.9818 - f1_score: 0.9817 - val_loss: 0.0447 - val_acc: 0.9574 - val_f1_score: 0.9593\n",
      "Epoch 12/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9823 - f1_score: 0.9823\n",
      "Epoch 00012: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 114s 425ms/step - loss: 0.0172 - acc: 0.9823 - f1_score: 0.9823 - val_loss: 0.0254 - val_acc: 0.9787 - val_f1_score: 0.9786\n",
      "Epoch 13/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9818 - f1_score: 0.9818\n",
      "Epoch 00013: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0169 - acc: 0.9818 - f1_score: 0.9818 - val_loss: 0.0261 - val_acc: 0.9748 - val_f1_score: 0.9748\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0163 - acc: 0.9839 - f1_score: 0.9838\n",
      "Epoch 00014: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 116s 433ms/step - loss: 0.0163 - acc: 0.9839 - f1_score: 0.9838 - val_loss: 0.0526 - val_acc: 0.9569 - val_f1_score: 0.9564\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0156 - acc: 0.9854 - f1_score: 0.9854\n",
      "Epoch 00015: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0156 - acc: 0.9854 - f1_score: 0.9854 - val_loss: 0.0265 - val_acc: 0.9784 - val_f1_score: 0.9781\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0126 - acc: 0.9869 - f1_score: 0.9869\n",
      "Epoch 00016: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0126 - acc: 0.9869 - f1_score: 0.9869 - val_loss: 0.1654 - val_acc: 0.9572 - val_f1_score: 0.9575\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0157 - acc: 0.9838 - f1_score: 0.9837\n",
      "Epoch 00017: val_f1_score did not improve from 0.98112\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0157 - acc: 0.9838 - f1_score: 0.9837 - val_loss: 0.0285 - val_acc: 0.9726 - val_f1_score: 0.9729\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0122 - acc: 0.9869 - f1_score: 0.9868\n",
      "Epoch 00018: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0122 - acc: 0.9869 - f1_score: 0.9868 - val_loss: 0.0262 - val_acc: 0.9768 - val_f1_score: 0.9762\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9865 - f1_score: 0.9865\n",
      "Epoch 00019: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0128 - acc: 0.9865 - f1_score: 0.9865 - val_loss: 0.0247 - val_acc: 0.9773 - val_f1_score: 0.9774\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9891 - f1_score: 0.9891Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.98112\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0111 - acc: 0.9891 - f1_score: 0.9891 - val_loss: 0.0317 - val_acc: 0.9760 - val_f1_score: 0.9759\n",
      "Epoch 00020: early stopping\n",
      "\n",
      "Starting 2 skf\n",
      "\n",
      "Epoch 1/100\n",
      "  2/269 [..............................] - ETA: 7:40 - loss: 0.6631 - acc: 0.2778 - f1_score: 0.2765WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 1.1163s vs `on_train_batch_end` time: 2.3369s). Check your callbacks.\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0972 - acc: 0.8965 - f1_score: 0.8953\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.91005, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 123s 457ms/step - loss: 0.0972 - acc: 0.8965 - f1_score: 0.8953 - val_loss: 0.0936 - val_acc: 0.9118 - val_f1_score: 0.9101\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0470 - acc: 0.9523 - f1_score: 0.9520\n",
      "Epoch 00002: val_f1_score improved from 0.91005 to 0.96454, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0470 - acc: 0.9523 - f1_score: 0.9520 - val_loss: 0.0391 - val_acc: 0.9649 - val_f1_score: 0.9645\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0373 - acc: 0.9630 - f1_score: 0.9628\n",
      "Epoch 00003: val_f1_score improved from 0.96454 to 0.96782, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0373 - acc: 0.9630 - f1_score: 0.9628 - val_loss: 0.0329 - val_acc: 0.9679 - val_f1_score: 0.9678\n",
      "Epoch 4/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0332 - acc: 0.9667 - f1_score: 0.9666\n",
      "Epoch 00004: val_f1_score improved from 0.96782 to 0.97036, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 116s 433ms/step - loss: 0.0332 - acc: 0.9667 - f1_score: 0.9666 - val_loss: 0.0302 - val_acc: 0.9707 - val_f1_score: 0.9704\n",
      "Epoch 5/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9710 - f1_score: 0.9708\n",
      "Epoch 00005: val_f1_score improved from 0.97036 to 0.97397, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0275 - acc: 0.9710 - f1_score: 0.9708 - val_loss: 0.0262 - val_acc: 0.9740 - val_f1_score: 0.9740\n",
      "Epoch 6/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0255 - acc: 0.9759 - f1_score: 0.9758\n",
      "Epoch 00006: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0255 - acc: 0.9759 - f1_score: 0.9758 - val_loss: 0.0307 - val_acc: 0.9721 - val_f1_score: 0.9717\n",
      "Epoch 7/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.9779 - f1_score: 0.9778\n",
      "Epoch 00007: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 111s 414ms/step - loss: 0.0224 - acc: 0.9779 - f1_score: 0.9778 - val_loss: 0.0290 - val_acc: 0.9713 - val_f1_score: 0.9711\n",
      "Epoch 8/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0210 - acc: 0.9777 - f1_score: 0.9777\n",
      "Epoch 00008: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 114s 423ms/step - loss: 0.0210 - acc: 0.9777 - f1_score: 0.9777 - val_loss: 0.0315 - val_acc: 0.9732 - val_f1_score: 0.9733\n",
      "Epoch 9/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0208 - acc: 0.9798 - f1_score: 0.9798\n",
      "Epoch 00009: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 112s 418ms/step - loss: 0.0208 - acc: 0.9798 - f1_score: 0.9798 - val_loss: 0.0339 - val_acc: 0.9666 - val_f1_score: 0.9660\n",
      "Epoch 10/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0203 - acc: 0.9790 - f1_score: 0.9790\n",
      "Epoch 00010: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0203 - acc: 0.9790 - f1_score: 0.9790 - val_loss: 0.0276 - val_acc: 0.9724 - val_f1_score: 0.9719\n",
      "Epoch 11/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9797 - f1_score: 0.9796\n",
      "Epoch 00011: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0193 - acc: 0.9797 - f1_score: 0.9796 - val_loss: 0.0334 - val_acc: 0.9713 - val_f1_score: 0.9711\n",
      "Epoch 12/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0174 - acc: 0.9828 - f1_score: 0.9828\n",
      "Epoch 00012: val_f1_score did not improve from 0.97397\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0174 - acc: 0.9828 - f1_score: 0.9828 - val_loss: 0.0327 - val_acc: 0.9721 - val_f1_score: 0.9721\n",
      "Epoch 13/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0165 - acc: 0.9822 - f1_score: 0.9822\n",
      "Epoch 00013: val_f1_score did not improve from 0.97397\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0165 - acc: 0.9822 - f1_score: 0.9822 - val_loss: 0.0340 - val_acc: 0.9679 - val_f1_score: 0.9677\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9857 - f1_score: 0.9857\n",
      "Epoch 00014: val_f1_score improved from 0.97397 to 0.97654, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 118s 438ms/step - loss: 0.0140 - acc: 0.9857 - f1_score: 0.9857 - val_loss: 0.0244 - val_acc: 0.9762 - val_f1_score: 0.9765\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.9834 - f1_score: 0.9834\n",
      "Epoch 00015: val_f1_score did not improve from 0.97654\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0158 - acc: 0.9834 - f1_score: 0.9834 - val_loss: 0.0235 - val_acc: 0.9762 - val_f1_score: 0.9762\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9880 - f1_score: 0.9879\n",
      "Epoch 00016: val_f1_score improved from 0.97654 to 0.98069, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 118s 437ms/step - loss: 0.0119 - acc: 0.9880 - f1_score: 0.9879 - val_loss: 0.0212 - val_acc: 0.9807 - val_f1_score: 0.9807\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9865 - f1_score: 0.9865\n",
      "Epoch 00017: val_f1_score did not improve from 0.98069\n",
      "269/269 [==============================] - 116s 429ms/step - loss: 0.0129 - acc: 0.9865 - f1_score: 0.9865 - val_loss: 0.0543 - val_acc: 0.9541 - val_f1_score: 0.9539\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9873 - f1_score: 0.9873\n",
      "Epoch 00018: val_f1_score did not improve from 0.98069\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0128 - acc: 0.9873 - f1_score: 0.9873 - val_loss: 0.0354 - val_acc: 0.9679 - val_f1_score: 0.9682\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0145 - acc: 0.9844 - f1_score: 0.9845\n",
      "Epoch 00019: val_f1_score did not improve from 0.98069\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0145 - acc: 0.9844 - f1_score: 0.9845 - val_loss: 0.0282 - val_acc: 0.9748 - val_f1_score: 0.9747\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9887 - f1_score: 0.9887\n",
      "Epoch 00020: val_f1_score did not improve from 0.98069\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0113 - acc: 0.9887 - f1_score: 0.9887 - val_loss: 0.0314 - val_acc: 0.9779 - val_f1_score: 0.9777\n",
      "Epoch 21/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9899 - f1_score: 0.9899\n",
      "Epoch 00021: val_f1_score did not improve from 0.98069\n",
      "269/269 [==============================] - 111s 412ms/step - loss: 0.0104 - acc: 0.9899 - f1_score: 0.9899 - val_loss: 0.0250 - val_acc: 0.9793 - val_f1_score: 0.9794\n",
      "Epoch 22/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9888 - f1_score: 0.9888\n",
      "Epoch 00022: val_f1_score improved from 0.98069 to 0.98077, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 118s 439ms/step - loss: 0.0098 - acc: 0.9888 - f1_score: 0.9888 - val_loss: 0.0260 - val_acc: 0.9809 - val_f1_score: 0.9808\n",
      "Epoch 23/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0100 - acc: 0.9904 - f1_score: 0.9904\n",
      "Epoch 00023: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0100 - acc: 0.9904 - f1_score: 0.9904 - val_loss: 0.0250 - val_acc: 0.9784 - val_f1_score: 0.9783\n",
      "Epoch 24/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9903 - f1_score: 0.9903\n",
      "Epoch 00024: val_f1_score did not improve from 0.98077\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0004050000192364678.\n",
      "269/269 [==============================] - 114s 423ms/step - loss: 0.0094 - acc: 0.9903 - f1_score: 0.9903 - val_loss: 0.0272 - val_acc: 0.9765 - val_f1_score: 0.9765\n",
      "Epoch 25/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9883 - f1_score: 0.9883\n",
      "Epoch 00025: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0104 - acc: 0.9883 - f1_score: 0.9883 - val_loss: 0.0261 - val_acc: 0.9782 - val_f1_score: 0.9781\n",
      "Epoch 26/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9893 - f1_score: 0.9893\n",
      "Epoch 00026: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 111s 412ms/step - loss: 0.0095 - acc: 0.9893 - f1_score: 0.9893 - val_loss: 0.0298 - val_acc: 0.9757 - val_f1_score: 0.9754\n",
      "Epoch 27/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9928 - f1_score: 0.9928\n",
      "Epoch 00027: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0076 - acc: 0.9928 - f1_score: 0.9928 - val_loss: 0.0343 - val_acc: 0.9721 - val_f1_score: 0.9719\n",
      "Epoch 28/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9913 - f1_score: 0.9913\n",
      "Epoch 00028: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0088 - acc: 0.9913 - f1_score: 0.9913 - val_loss: 0.0397 - val_acc: 0.9701 - val_f1_score: 0.9694\n",
      "Epoch 29/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0085 - acc: 0.9909 - f1_score: 0.9909\n",
      "Epoch 00029: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0085 - acc: 0.9909 - f1_score: 0.9909 - val_loss: 0.0280 - val_acc: 0.9807 - val_f1_score: 0.9808\n",
      "Epoch 30/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9932 - f1_score: 0.9932\n",
      "Epoch 00030: val_f1_score did not improve from 0.98077\n",
      "269/269 [==============================] - 114s 424ms/step - loss: 0.0065 - acc: 0.9932 - f1_score: 0.9932 - val_loss: 0.0282 - val_acc: 0.9782 - val_f1_score: 0.9777\n",
      "Epoch 31/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9933 - f1_score: 0.9933\n",
      "Epoch 00031: val_f1_score improved from 0.98077 to 0.98358, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_3.h5\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0066 - acc: 0.9933 - f1_score: 0.9933 - val_loss: 0.0317 - val_acc: 0.9837 - val_f1_score: 0.9836\n",
      "Epoch 32/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0093 - acc: 0.9911 - f1_score: 0.9910\n",
      "Epoch 00032: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0093 - acc: 0.9911 - f1_score: 0.9910 - val_loss: 0.0275 - val_acc: 0.9798 - val_f1_score: 0.9796\n",
      "Epoch 33/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0071 - acc: 0.9930 - f1_score: 0.9930\n",
      "Epoch 00033: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0071 - acc: 0.9930 - f1_score: 0.9930 - val_loss: 0.0263 - val_acc: 0.9784 - val_f1_score: 0.9786\n",
      "Epoch 34/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9945 - f1_score: 0.9945\n",
      "Epoch 00034: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0058 - acc: 0.9945 - f1_score: 0.9945 - val_loss: 0.0355 - val_acc: 0.9746 - val_f1_score: 0.9742\n",
      "Epoch 35/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9910 - f1_score: 0.9910\n",
      "Epoch 00035: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 116s 433ms/step - loss: 0.0080 - acc: 0.9910 - f1_score: 0.9910 - val_loss: 0.0261 - val_acc: 0.9782 - val_f1_score: 0.9780\n",
      "Epoch 36/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9926 - f1_score: 0.9926\n",
      "Epoch 00036: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0076 - acc: 0.9926 - f1_score: 0.9926 - val_loss: 0.0305 - val_acc: 0.9790 - val_f1_score: 0.9790\n",
      "Epoch 37/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9937 - f1_score: 0.9937\n",
      "Epoch 00037: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0064 - acc: 0.9937 - f1_score: 0.9937 - val_loss: 0.0357 - val_acc: 0.9746 - val_f1_score: 0.9743\n",
      "Epoch 38/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9935 - f1_score: 0.9935\n",
      "Epoch 00038: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0068 - acc: 0.9935 - f1_score: 0.9935 - val_loss: 0.0288 - val_acc: 0.9809 - val_f1_score: 0.9810\n",
      "Epoch 39/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0074 - acc: 0.9917 - f1_score: 0.9917\n",
      "Epoch 00039: val_f1_score did not improve from 0.98358\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0003645000251708552.\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0074 - acc: 0.9917 - f1_score: 0.9917 - val_loss: 0.0328 - val_acc: 0.9751 - val_f1_score: 0.9753\n",
      "Epoch 40/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9940 - f1_score: 0.9940\n",
      "Epoch 00040: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0062 - acc: 0.9940 - f1_score: 0.9940 - val_loss: 0.0370 - val_acc: 0.9713 - val_f1_score: 0.9713\n",
      "Epoch 41/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9937 - f1_score: 0.9937\n",
      "Epoch 00041: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 112s 417ms/step - loss: 0.0054 - acc: 0.9937 - f1_score: 0.9937 - val_loss: 0.0282 - val_acc: 0.9798 - val_f1_score: 0.9799\n",
      "Epoch 42/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9936 - f1_score: 0.9936Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00042: val_f1_score did not improve from 0.98358\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0068 - acc: 0.9936 - f1_score: 0.9936 - val_loss: 0.0354 - val_acc: 0.9762 - val_f1_score: 0.9761\n",
      "Epoch 00042: early stopping\n",
      "\n",
      "Starting 3 skf\n",
      "\n",
      "Epoch 1/100\n",
      "  2/269 [..............................] - ETA: 5:44 - loss: 0.6646 - acc: 0.2222 - f1_score: 0.2072WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3810s vs `on_train_batch_end` time: 2.2038s). Check your callbacks.\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0948 - acc: 0.9016 - f1_score: 0.9006\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.77430, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 121s 451ms/step - loss: 0.0948 - acc: 0.9016 - f1_score: 0.9006 - val_loss: 0.1915 - val_acc: 0.7991 - val_f1_score: 0.7743\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0455 - acc: 0.9541 - f1_score: 0.9537\n",
      "Epoch 00002: val_f1_score improved from 0.77430 to 0.93436, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0455 - acc: 0.9541 - f1_score: 0.9537 - val_loss: 0.0845 - val_acc: 0.9345 - val_f1_score: 0.9344\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0394 - acc: 0.9610 - f1_score: 0.9607\n",
      "Epoch 00003: val_f1_score improved from 0.93436 to 0.97547, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 118s 440ms/step - loss: 0.0394 - acc: 0.9610 - f1_score: 0.9607 - val_loss: 0.0252 - val_acc: 0.9757 - val_f1_score: 0.9755\n",
      "Epoch 4/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0325 - acc: 0.9664 - f1_score: 0.9663\n",
      "Epoch 00004: val_f1_score did not improve from 0.97547\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0325 - acc: 0.9664 - f1_score: 0.9663 - val_loss: 0.0547 - val_acc: 0.9525 - val_f1_score: 0.9493\n",
      "Epoch 5/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0294 - acc: 0.9722 - f1_score: 0.9721\n",
      "Epoch 00005: val_f1_score did not improve from 0.97547\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0294 - acc: 0.9722 - f1_score: 0.9721 - val_loss: 0.0302 - val_acc: 0.9701 - val_f1_score: 0.9698\n",
      "Epoch 6/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0282 - acc: 0.9726 - f1_score: 0.9725\n",
      "Epoch 00006: val_f1_score did not improve from 0.97547\n",
      "269/269 [==============================] - 114s 424ms/step - loss: 0.0282 - acc: 0.9726 - f1_score: 0.9725 - val_loss: 0.0305 - val_acc: 0.9737 - val_f1_score: 0.9731\n",
      "Epoch 7/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.9760 - f1_score: 0.9760\n",
      "Epoch 00007: val_f1_score improved from 0.97547 to 0.97811, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0239 - acc: 0.9760 - f1_score: 0.9760 - val_loss: 0.0213 - val_acc: 0.9782 - val_f1_score: 0.9781\n",
      "Epoch 8/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0238 - acc: 0.9754 - f1_score: 0.9753\n",
      "Epoch 00008: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0238 - acc: 0.9754 - f1_score: 0.9753 - val_loss: 0.0274 - val_acc: 0.9690 - val_f1_score: 0.9696\n",
      "Epoch 9/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9788 - f1_score: 0.9787\n",
      "Epoch 00009: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0204 - acc: 0.9788 - f1_score: 0.9787 - val_loss: 0.0459 - val_acc: 0.9566 - val_f1_score: 0.9549\n",
      "Epoch 10/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0178 - acc: 0.9810 - f1_score: 0.9810\n",
      "Epoch 00010: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0178 - acc: 0.9810 - f1_score: 0.9810 - val_loss: 0.0350 - val_acc: 0.9693 - val_f1_score: 0.9685\n",
      "Epoch 11/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9791 - f1_score: 0.9791\n",
      "Epoch 00011: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 111s 412ms/step - loss: 0.0204 - acc: 0.9791 - f1_score: 0.9791 - val_loss: 0.0316 - val_acc: 0.9743 - val_f1_score: 0.9739\n",
      "Epoch 12/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0171 - acc: 0.9826 - f1_score: 0.9825\n",
      "Epoch 00012: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0171 - acc: 0.9826 - f1_score: 0.9825 - val_loss: 0.0284 - val_acc: 0.9715 - val_f1_score: 0.9712\n",
      "Epoch 13/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0149 - acc: 0.9833 - f1_score: 0.9833\n",
      "Epoch 00013: val_f1_score did not improve from 0.97811\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0149 - acc: 0.9833 - f1_score: 0.9833 - val_loss: 0.0405 - val_acc: 0.9713 - val_f1_score: 0.9707\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0184 - acc: 0.9802 - f1_score: 0.9802\n",
      "Epoch 00014: val_f1_score improved from 0.97811 to 0.97915, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0184 - acc: 0.9802 - f1_score: 0.9802 - val_loss: 0.0268 - val_acc: 0.9795 - val_f1_score: 0.9792\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0154 - acc: 0.9837 - f1_score: 0.9837\n",
      "Epoch 00015: val_f1_score did not improve from 0.97915\n",
      "269/269 [==============================] - 117s 436ms/step - loss: 0.0154 - acc: 0.9837 - f1_score: 0.9837 - val_loss: 0.0344 - val_acc: 0.9701 - val_f1_score: 0.9700\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0135 - acc: 0.9862 - f1_score: 0.9861\n",
      "Epoch 00016: val_f1_score did not improve from 0.97915\n",
      "269/269 [==============================] - 113s 422ms/step - loss: 0.0135 - acc: 0.9862 - f1_score: 0.9861 - val_loss: 0.0324 - val_acc: 0.9721 - val_f1_score: 0.9715\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9833 - f1_score: 0.9833\n",
      "Epoch 00017: val_f1_score did not improve from 0.97915\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0161 - acc: 0.9833 - f1_score: 0.9833 - val_loss: 0.0360 - val_acc: 0.9666 - val_f1_score: 0.9662\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9867 - f1_score: 0.9867\n",
      "Epoch 00018: val_f1_score did not improve from 0.97915\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0136 - acc: 0.9867 - f1_score: 0.9867 - val_loss: 0.0258 - val_acc: 0.9773 - val_f1_score: 0.9771\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9869 - f1_score: 0.9869\n",
      "Epoch 00019: val_f1_score did not improve from 0.97915\n",
      "269/269 [==============================] - 119s 444ms/step - loss: 0.0121 - acc: 0.9869 - f1_score: 0.9869 - val_loss: 0.0194 - val_acc: 0.9779 - val_f1_score: 0.9778\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9866 - f1_score: 0.9866\n",
      "Epoch 00020: val_f1_score improved from 0.97915 to 0.98067, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 117s 436ms/step - loss: 0.0128 - acc: 0.9866 - f1_score: 0.9866 - val_loss: 0.0182 - val_acc: 0.9809 - val_f1_score: 0.9807\n",
      "Epoch 21/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9860 - f1_score: 0.9859\n",
      "Epoch 00021: val_f1_score improved from 0.98067 to 0.98151, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 117s 436ms/step - loss: 0.0134 - acc: 0.9860 - f1_score: 0.9859 - val_loss: 0.0218 - val_acc: 0.9815 - val_f1_score: 0.9815\n",
      "Epoch 22/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9877 - f1_score: 0.9877\n",
      "Epoch 00022: val_f1_score did not improve from 0.98151\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0121 - acc: 0.9877 - f1_score: 0.9877 - val_loss: 0.0306 - val_acc: 0.9740 - val_f1_score: 0.9734\n",
      "Epoch 23/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0143 - acc: 0.9855 - f1_score: 0.9855\n",
      "Epoch 00023: val_f1_score did not improve from 0.98151\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0143 - acc: 0.9855 - f1_score: 0.9855 - val_loss: 0.0225 - val_acc: 0.9798 - val_f1_score: 0.9796\n",
      "Epoch 24/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0112 - acc: 0.9868 - f1_score: 0.9868\n",
      "Epoch 00024: val_f1_score did not improve from 0.98151\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0112 - acc: 0.9868 - f1_score: 0.9868 - val_loss: 0.0304 - val_acc: 0.9713 - val_f1_score: 0.9711\n",
      "Epoch 25/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0105 - acc: 0.9891 - f1_score: 0.9890\n",
      "Epoch 00025: val_f1_score did not improve from 0.98151\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0105 - acc: 0.9891 - f1_score: 0.9890 - val_loss: 0.0255 - val_acc: 0.9790 - val_f1_score: 0.9787\n",
      "Epoch 26/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0106 - acc: 0.9908 - f1_score: 0.9908\n",
      "Epoch 00026: val_f1_score improved from 0.98151 to 0.98165, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_4.h5\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0106 - acc: 0.9908 - f1_score: 0.9908 - val_loss: 0.0234 - val_acc: 0.9818 - val_f1_score: 0.9817\n",
      "Epoch 27/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9893 - f1_score: 0.9892\n",
      "Epoch 00027: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0101 - acc: 0.9893 - f1_score: 0.9892 - val_loss: 0.0212 - val_acc: 0.9809 - val_f1_score: 0.9810\n",
      "Epoch 28/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9904 - f1_score: 0.9904\n",
      "Epoch 00028: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0102 - acc: 0.9904 - f1_score: 0.9904 - val_loss: 0.0224 - val_acc: 0.9801 - val_f1_score: 0.9801\n",
      "Epoch 29/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0127 - acc: 0.9873 - f1_score: 0.9873\n",
      "Epoch 00029: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0127 - acc: 0.9873 - f1_score: 0.9873 - val_loss: 0.0328 - val_acc: 0.9760 - val_f1_score: 0.9754\n",
      "Epoch 30/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9888 - f1_score: 0.9888\n",
      "Epoch 00030: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0113 - acc: 0.9888 - f1_score: 0.9888 - val_loss: 0.0254 - val_acc: 0.9782 - val_f1_score: 0.9780\n",
      "Epoch 31/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9922 - f1_score: 0.9921\n",
      "Epoch 00031: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0072 - acc: 0.9922 - f1_score: 0.9921 - val_loss: 0.0300 - val_acc: 0.9726 - val_f1_score: 0.9733\n",
      "Epoch 32/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0108 - acc: 0.9887 - f1_score: 0.9887\n",
      "Epoch 00032: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0108 - acc: 0.9887 - f1_score: 0.9887 - val_loss: 0.0276 - val_acc: 0.9793 - val_f1_score: 0.9790\n",
      "Epoch 33/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9905 - f1_score: 0.9905\n",
      "Epoch 00033: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0096 - acc: 0.9905 - f1_score: 0.9905 - val_loss: 0.0277 - val_acc: 0.9784 - val_f1_score: 0.9779\n",
      "Epoch 34/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0105 - acc: 0.9899 - f1_score: 0.9899\n",
      "Epoch 00034: val_f1_score did not improve from 0.98165\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0105 - acc: 0.9899 - f1_score: 0.9899 - val_loss: 0.0237 - val_acc: 0.9773 - val_f1_score: 0.9772\n",
      "Epoch 35/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0089 - acc: 0.9912 - f1_score: 0.9912\n",
      "Epoch 00035: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0089 - acc: 0.9912 - f1_score: 0.9912 - val_loss: 0.0273 - val_acc: 0.9751 - val_f1_score: 0.9750\n",
      "Epoch 36/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9917 - f1_score: 0.9917\n",
      "Epoch 00036: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 112s 418ms/step - loss: 0.0088 - acc: 0.9917 - f1_score: 0.9917 - val_loss: 0.0265 - val_acc: 0.9782 - val_f1_score: 0.9781\n",
      "Epoch 37/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0085 - acc: 0.9921 - f1_score: 0.9921Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00037: val_f1_score did not improve from 0.98165\n",
      "269/269 [==============================] - 110s 409ms/step - loss: 0.0085 - acc: 0.9921 - f1_score: 0.9921 - val_loss: 0.0375 - val_acc: 0.9737 - val_f1_score: 0.9736\n",
      "Epoch 00037: early stopping\n",
      "\n",
      "Starting 4 skf\n",
      "\n",
      "Epoch 1/100\n",
      "  2/269 [..............................] - ETA: 6:28 - loss: 0.6699 - acc: 0.1667 - f1_score: 0.1772WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4768s vs `on_train_batch_end` time: 2.4343s). Check your callbacks.\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.1015 - acc: 0.8918 - f1_score: 0.8903\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.91942, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 123s 456ms/step - loss: 0.1015 - acc: 0.8918 - f1_score: 0.8903 - val_loss: 0.0805 - val_acc: 0.9212 - val_f1_score: 0.9194\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0485 - acc: 0.9521 - f1_score: 0.9518\n",
      "Epoch 00002: val_f1_score improved from 0.91942 to 0.97356, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0485 - acc: 0.9521 - f1_score: 0.9518 - val_loss: 0.0250 - val_acc: 0.9743 - val_f1_score: 0.9736\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.9592 - f1_score: 0.9589\n",
      "Epoch 00003: val_f1_score did not improve from 0.97356\n",
      "269/269 [==============================] - 109s 407ms/step - loss: 0.0393 - acc: 0.9592 - f1_score: 0.9589 - val_loss: 0.0397 - val_acc: 0.9602 - val_f1_score: 0.9601\n",
      "Epoch 4/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9658 - f1_score: 0.9656\n",
      "Epoch 00004: val_f1_score improved from 0.97356 to 0.97475, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 118s 437ms/step - loss: 0.0329 - acc: 0.9658 - f1_score: 0.9656 - val_loss: 0.0242 - val_acc: 0.9748 - val_f1_score: 0.9747\n",
      "Epoch 5/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0289 - acc: 0.9717 - f1_score: 0.9716\n",
      "Epoch 00005: val_f1_score did not improve from 0.97475\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0289 - acc: 0.9717 - f1_score: 0.9716 - val_loss: 0.0258 - val_acc: 0.9715 - val_f1_score: 0.9714\n",
      "Epoch 6/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0270 - acc: 0.9726 - f1_score: 0.9725\n",
      "Epoch 00006: val_f1_score improved from 0.97475 to 0.97674, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 118s 439ms/step - loss: 0.0270 - acc: 0.9726 - f1_score: 0.9725 - val_loss: 0.0216 - val_acc: 0.9768 - val_f1_score: 0.9767\n",
      "Epoch 7/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0251 - acc: 0.9730 - f1_score: 0.9730\n",
      "Epoch 00007: val_f1_score improved from 0.97674 to 0.98238, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0251 - acc: 0.9730 - f1_score: 0.9730 - val_loss: 0.0168 - val_acc: 0.9823 - val_f1_score: 0.9824\n",
      "Epoch 8/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0208 - acc: 0.9792 - f1_score: 0.9791\n",
      "Epoch 00008: val_f1_score did not improve from 0.98238\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0208 - acc: 0.9792 - f1_score: 0.9791 - val_loss: 0.0201 - val_acc: 0.9776 - val_f1_score: 0.9775\n",
      "Epoch 9/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0231 - acc: 0.9761 - f1_score: 0.9761\n",
      "Epoch 00009: val_f1_score did not improve from 0.98238\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0231 - acc: 0.9761 - f1_score: 0.9761 - val_loss: 0.0181 - val_acc: 0.9812 - val_f1_score: 0.9813\n",
      "Epoch 10/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0194 - acc: 0.9805 - f1_score: 0.9805\n",
      "Epoch 00010: val_f1_score did not improve from 0.98238\n",
      "269/269 [==============================] - 110s 409ms/step - loss: 0.0194 - acc: 0.9805 - f1_score: 0.9805 - val_loss: 0.0226 - val_acc: 0.9740 - val_f1_score: 0.9733\n",
      "Epoch 11/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0187 - acc: 0.9800 - f1_score: 0.9800\n",
      "Epoch 00011: val_f1_score improved from 0.98238 to 0.98349, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 114s 425ms/step - loss: 0.0187 - acc: 0.9800 - f1_score: 0.9800 - val_loss: 0.0191 - val_acc: 0.9837 - val_f1_score: 0.9835\n",
      "Epoch 12/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0184 - acc: 0.9815 - f1_score: 0.9814\n",
      "Epoch 00012: val_f1_score did not improve from 0.98349\n",
      "269/269 [==============================] - 111s 411ms/step - loss: 0.0184 - acc: 0.9815 - f1_score: 0.9814 - val_loss: 0.0183 - val_acc: 0.9818 - val_f1_score: 0.9819\n",
      "Epoch 13/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9829 - f1_score: 0.9829\n",
      "Epoch 00013: val_f1_score did not improve from 0.98349\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0175 - acc: 0.9829 - f1_score: 0.9829 - val_loss: 0.0205 - val_acc: 0.9823 - val_f1_score: 0.9818\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9832 - f1_score: 0.9832\n",
      "Epoch 00014: val_f1_score did not improve from 0.98349\n",
      "269/269 [==============================] - 117s 435ms/step - loss: 0.0169 - acc: 0.9832 - f1_score: 0.9832 - val_loss: 0.0193 - val_acc: 0.9801 - val_f1_score: 0.9796\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9818 - f1_score: 0.9817\n",
      "Epoch 00015: val_f1_score did not improve from 0.98349\n",
      "269/269 [==============================] - 114s 423ms/step - loss: 0.0173 - acc: 0.9818 - f1_score: 0.9817 - val_loss: 0.0225 - val_acc: 0.9740 - val_f1_score: 0.9742\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0150 - acc: 0.9836 - f1_score: 0.9836\n",
      "Epoch 00016: val_f1_score did not improve from 0.98349\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0150 - acc: 0.9836 - f1_score: 0.9836 - val_loss: 0.0207 - val_acc: 0.9795 - val_f1_score: 0.9793\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9860 - f1_score: 0.9860\n",
      "Epoch 00017: val_f1_score improved from 0.98349 to 0.98359, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 118s 438ms/step - loss: 0.0140 - acc: 0.9860 - f1_score: 0.9860 - val_loss: 0.0171 - val_acc: 0.9837 - val_f1_score: 0.9836\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9860 - f1_score: 0.9859\n",
      "Epoch 00018: val_f1_score did not improve from 0.98359\n",
      "269/269 [==============================] - 111s 412ms/step - loss: 0.0134 - acc: 0.9860 - f1_score: 0.9859 - val_loss: 0.0176 - val_acc: 0.9818 - val_f1_score: 0.9819\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0156 - acc: 0.9849 - f1_score: 0.9849\n",
      "Epoch 00019: val_f1_score did not improve from 0.98359\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0156 - acc: 0.9849 - f1_score: 0.9849 - val_loss: 0.0313 - val_acc: 0.9729 - val_f1_score: 0.9726\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9868 - f1_score: 0.9867\n",
      "Epoch 00020: val_f1_score improved from 0.98359 to 0.98445, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0128 - acc: 0.9868 - f1_score: 0.9867 - val_loss: 0.0176 - val_acc: 0.9845 - val_f1_score: 0.9844\n",
      "Epoch 21/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0131 - acc: 0.9866 - f1_score: 0.9866\n",
      "Epoch 00021: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 115s 428ms/step - loss: 0.0131 - acc: 0.9866 - f1_score: 0.9866 - val_loss: 0.0202 - val_acc: 0.9812 - val_f1_score: 0.9810\n",
      "Epoch 22/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0123 - acc: 0.9864 - f1_score: 0.9864\n",
      "Epoch 00022: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 111s 413ms/step - loss: 0.0123 - acc: 0.9864 - f1_score: 0.9864 - val_loss: 0.0156 - val_acc: 0.9842 - val_f1_score: 0.9841\n",
      "Epoch 23/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0118 - acc: 0.9888 - f1_score: 0.9888\n",
      "Epoch 00023: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0118 - acc: 0.9888 - f1_score: 0.9888 - val_loss: 0.0259 - val_acc: 0.9773 - val_f1_score: 0.9768\n",
      "Epoch 24/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9888 - f1_score: 0.9888\n",
      "Epoch 00024: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0111 - acc: 0.9888 - f1_score: 0.9888 - val_loss: 0.0244 - val_acc: 0.9798 - val_f1_score: 0.9791\n",
      "Epoch 25/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9895 - f1_score: 0.9895\n",
      "Epoch 00025: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 113s 422ms/step - loss: 0.0102 - acc: 0.9895 - f1_score: 0.9895 - val_loss: 0.0191 - val_acc: 0.9820 - val_f1_score: 0.9821\n",
      "Epoch 26/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9882 - f1_score: 0.9882\n",
      "Epoch 00026: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 115s 426ms/step - loss: 0.0104 - acc: 0.9882 - f1_score: 0.9882 - val_loss: 0.0237 - val_acc: 0.9818 - val_f1_score: 0.9817\n",
      "Epoch 27/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0109 - acc: 0.9884 - f1_score: 0.9884\n",
      "Epoch 00027: val_f1_score did not improve from 0.98445\n",
      "269/269 [==============================] - 113s 421ms/step - loss: 0.0109 - acc: 0.9884 - f1_score: 0.9884 - val_loss: 0.0237 - val_acc: 0.9787 - val_f1_score: 0.9785\n",
      "Epoch 28/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0108 - acc: 0.9893 - f1_score: 0.9893\n",
      "Epoch 00028: val_f1_score improved from 0.98445 to 0.98542, saving model to /app/_data/models/final/without_wrong_pred/eff4_0858_without_wrong_pred_5.h5\n",
      "269/269 [==============================] - 118s 437ms/step - loss: 0.0108 - acc: 0.9893 - f1_score: 0.9893 - val_loss: 0.0166 - val_acc: 0.9854 - val_f1_score: 0.9854\n",
      "Epoch 29/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9884 - f1_score: 0.9884\n",
      "Epoch 00029: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 116s 431ms/step - loss: 0.0111 - acc: 0.9884 - f1_score: 0.9884 - val_loss: 0.0176 - val_acc: 0.9851 - val_f1_score: 0.9850\n",
      "Epoch 30/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9909 - f1_score: 0.9909\n",
      "Epoch 00030: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 114s 424ms/step - loss: 0.0096 - acc: 0.9909 - f1_score: 0.9909 - val_loss: 0.0155 - val_acc: 0.9848 - val_f1_score: 0.9847\n",
      "Epoch 31/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9911 - f1_score: 0.9911\n",
      "Epoch 00031: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 115s 429ms/step - loss: 0.0088 - acc: 0.9911 - f1_score: 0.9911 - val_loss: 0.0219 - val_acc: 0.9790 - val_f1_score: 0.9789\n",
      "Epoch 32/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0114 - acc: 0.9895 - f1_score: 0.9896\n",
      "Epoch 00032: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0114 - acc: 0.9895 - f1_score: 0.9896 - val_loss: 0.0158 - val_acc: 0.9851 - val_f1_score: 0.9850\n",
      "Epoch 33/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0097 - acc: 0.9899 - f1_score: 0.9900\n",
      "Epoch 00033: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 117s 434ms/step - loss: 0.0097 - acc: 0.9899 - f1_score: 0.9900 - val_loss: 0.0165 - val_acc: 0.9845 - val_f1_score: 0.9845\n",
      "Epoch 34/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9882 - f1_score: 0.9882\n",
      "Epoch 00034: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 115s 427ms/step - loss: 0.0121 - acc: 0.9882 - f1_score: 0.9882 - val_loss: 0.0193 - val_acc: 0.9801 - val_f1_score: 0.9802\n",
      "Epoch 35/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0089 - acc: 0.9908 - f1_score: 0.9908\n",
      "Epoch 00035: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 114s 425ms/step - loss: 0.0089 - acc: 0.9908 - f1_score: 0.9908 - val_loss: 0.0195 - val_acc: 0.9823 - val_f1_score: 0.9821\n",
      "Epoch 36/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9913 - f1_score: 0.9913\n",
      "Epoch 00036: val_f1_score did not improve from 0.98542\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "269/269 [==============================] - 116s 433ms/step - loss: 0.0088 - acc: 0.9913 - f1_score: 0.9913 - val_loss: 0.0187 - val_acc: 0.9842 - val_f1_score: 0.9843\n",
      "Epoch 37/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9927 - f1_score: 0.9927\n",
      "Epoch 00037: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 114s 423ms/step - loss: 0.0073 - acc: 0.9927 - f1_score: 0.9927 - val_loss: 0.0133 - val_acc: 0.9854 - val_f1_score: 0.9853\n",
      "Epoch 38/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9932 - f1_score: 0.9932\n",
      "Epoch 00038: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 116s 432ms/step - loss: 0.0068 - acc: 0.9932 - f1_score: 0.9932 - val_loss: 0.0188 - val_acc: 0.9845 - val_f1_score: 0.9844\n",
      "Epoch 39/100\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9910 - f1_score: 0.9910Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00039: val_f1_score did not improve from 0.98542\n",
      "269/269 [==============================] - 116s 430ms/step - loss: 0.0086 - acc: 0.9910 - f1_score: 0.9910 - val_loss: 0.0221 - val_acc: 0.9815 - val_f1_score: 0.9811\n",
      "Epoch 00039: early stopping\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(\n",
    "    skf.split(df_labels[\"image\"], df_labels[\"labels\"])\n",
    "):\n",
    "    train, valid = df_labels.loc[train_index], df_labels.loc[valid_index]\n",
    "    print('\\nStarting '+str(i)+' skf\\n')\n",
    "    model_name = \"eff4_0858_without_wrong_pred_\" + str(i + 1) + \".h5\"\n",
    "    log_dir = 'logs_858_wwp_'+str(i + 1)+'/'\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_f1_score\",\n",
    "            patience=11,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"/app/_data/models/final/without_wrong_pred/\" + model_name,\n",
    "            monitor=\"val_f1_score\",\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode=\"max\",\n",
    "            save_freq=\"epoch\",\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_f1_score\",\n",
    "            factor=0.9,\n",
    "            patience=8,\n",
    "            verbose=1,\n",
    "            mode=\"max\",\n",
    "            min_delta=1e-4,\n",
    "            min_lr=0.00000001,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=\"/app/.tensorboard/\"+log_dir, histogram_freq=0\n",
    "        ),\n",
    "        keras.callbacks.experimental.BackupAndRestore(\n",
    "    '/app/_data/models/final/without_wrong_pred/backup/'\n",
    "),\n",
    "#         TensorBoard_Logger(log_dir=\"/app/.tensorboard/\"+log_dir, histogram_freq=0)\n",
    "    ]\n",
    "\n",
    "    gen_train = Generator(\n",
    "        df=train,\n",
    "        images_src_dir=TRAIN_IMG_PATH,\n",
    "        target_image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        augment=True,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "    )\n",
    "    gen_valid = Generator(\n",
    "        df=valid,\n",
    "        images_src_dir=TRAIN_IMG_PATH,\n",
    "        target_image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        augment=False,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "    )\n",
    "    model = get_model()\n",
    "    history = model.fit(\n",
    "        gen_train,\n",
    "        validation_data=gen_valid,\n",
    "        epochs=100,\n",
    "        steps_per_epoch=train.shape[0]//BATCH_SIZE,\n",
    "        validation_steps=valid.shape[0]//BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        workers = 10,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(file_path):\n",
    "    img = tf.io.read_file(TRAIN_IMG_PATH + file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def predict_new(path, model):\n",
    "    img = parse_all(path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    pred = model.predict(img)\n",
    "    return pred_to_labels(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  image                   labels\n",
      "0  bfc6d90f402f4c34.jpg  frog_eye_leaf_spot scab\n",
      "1  9eb93fe282326266.jpg           powdery_mildew\n",
      "2  f4cb3a8f41b413e4.jpg       frog_eye_leaf_spot\n",
      "3  98322eab16bef2c1.jpg                     rust\n",
      "4  dad5d6250cae80b7.jpg                  complex\n"
     ]
    }
   ],
   "source": [
    "df_sub = pd.DataFrame(columns=[\"image\", \"labels\"])\n",
    "for img_name in os.listdir(TRAIN_IMG_PATH):\n",
    "    pred = predict_new(img_name, model)\n",
    "\n",
    "    df_sub = df_sub.append({\"image\": img_name, \"labels\": pred}, ignore_index=True)\n",
    "\n",
    "print(df_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.merge(\n",
    "    labels_21_20[[\"image\", \"labels\"]],\n",
    "    on=\"image\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_pred\", \"_true\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"/app/sandbox/wrong_predictions/eff4/eff4_ns_cropped_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complex                    22\n",
       "scab                       10\n",
       "rust                        7\n",
       "frog_eye_leaf_spot          5\n",
       "scab frog_eye_leaf_spot     1\n",
       "rust frog_eye_leaf_spot     1\n",
       "powdery_mildew complex      1\n",
       "powdery_mildew              1\n",
       "healthy                     1\n",
       "Name: labels_true, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub[\"labels_pred\"] == \"\"][\"labels_true\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab frog_eye_leaf_spot            682\n",
       "complex                            438\n",
       "scab frog_eye_leaf_spot complex    200\n",
       "frog_eye_leaf_spot complex         165\n",
       "scab                               124\n",
       "rust frog_eye_leaf_spot            118\n",
       "rust complex                        91\n",
       "powdery_mildew complex              87\n",
       "rust                                74\n",
       "frog_eye_leaf_spot                  71\n",
       "healthy                             19\n",
       "powdery_mildew                       7\n",
       "Name: labels_true, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub[\"labels_pred\"] != df_sub[\"labels_true\"]][\"labels_true\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
