{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n",
      "Data package template written to: /app/_data/models/final/f1_sample/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets init -p /app/_data/models/final/f1_sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /app/kaggle.json'\n",
      "Starting upload for file eff4_0891_f1_5.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:10<00:00, 686kB/s]\n",
      "Upload successful: eff4_0891_f1_5.h5 (203MB)\n",
      "Starting upload for file eff4_0891_f1_3.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:09<00:00, 687kB/s]\n",
      "Upload successful: eff4_0891_f1_3.h5 (203MB)\n",
      "Starting upload for file eff4_0891_f1_4.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:09<00:00, 688kB/s]\n",
      "Upload successful: eff4_0891_f1_4.h5 (203MB)\n",
      "Starting upload for file eff4_0891_f1_1.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [05:25<00:00, 654kB/s]\n",
      "Upload successful: eff4_0891_f1_1.h5 (203MB)\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file eff4_0891_f1_2.h5\n",
      "100%|█████████████████████████████████████████| 203M/203M [09:35<00:00, 370kB/s]\n",
      "Upload successful: eff4_0891_f1_2.h5 (203MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/nataliayurasova/f1Sample\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets create -p /app/_data/models/final/f1_sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.891\n",
    "MODEL_BB_PATH= '../input/model-bb-1/bond_box_999_200.h5'\n",
    "MODEL_PATH = '../input/0865fulltrain/'\n",
    "IMAGE_SIZE = (380, 380)\n",
    "DF_PART = '../input/df-kf-plant/df_kf.csv'\n",
    "PATH = \"/kaggle/input/plant-pathology-2021-fgvc8/\"\n",
    "TRAIN_IMG_PATH = PATH+'train_images/'\n",
    "TEST_IMG_PATH = PATH+'test_images/'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES=6\n",
    "SEED = 1488\n",
    "- replace ''-'scab'\n",
    "https://www.kaggle.com/nataliayurasova/plant-pathology0891/edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (0.5.2)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.17.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations) (8.1.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.8/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install albumentations\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    train_test_split,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, EfficientNetB7\n",
    "from tensorflow.keras.layers import (\n",
    "    AveragePooling2D,\n",
    "    AvgPool2D,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import notebook, tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/app/_data/\"\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 380\n",
    "NUM_CLASSES = 6\n",
    "TRAIN_IMG_PATH = \"/app/_data/380_npy/\"\n",
    "TEST_IMG_PATH = \"/app/_data/test_images/\"\n",
    "feature_columns = [\n",
    "    \"complex\",\n",
    "    \"frog_eye_leaf_spot\",\n",
    "    \"healthy\",\n",
    "    \"powdery_mildew\",\n",
    "    \"rust\",\n",
    "    \"scab\",\n",
    "]\n",
    "wrong = ['ead085dfac287263.jpg', '95276ccd226ad933.jpg',\"da8770e819d2696d.jpg\", 'cd3a1d64e6806eb5.jpg', 'ccec54723ff91860.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(\"../_data/df_csv/labels_21_20.csv\", index_col=[0])\n",
    "df_labels = df_labels.query('image not in @wrong').reset_index(drop=True)\n",
    "df_labels[\"image\"] = df_labels[\"image\"].str.replace(\".jpg\", \".npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df_labels.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20225, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = labels_21_20.copy()\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.07, random_state=SEED)\n",
    "\n",
    "# for train_index, valid_index in sss.split(df[\"image\"], df[\"labels\"]):\n",
    "#     train, valid = df.loc[train_index], df.loc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 380*380\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.CLAHE(p=0.1, clip_limit=(1, 2), tile_grid_size=(8, 8)),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.MotionBlur((3, 3)),\n",
    "                albumentations.MedianBlur(blur_limit=3),\n",
    "                albumentations.GaussianBlur(blur_limit=(3, 3), sigma_limit=0),\n",
    "                albumentations.Blur(blur_limit=(3, 3)),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.GaussNoise(var_limit=[10, 50], mean=1),\n",
    "                albumentations.ISONoise(intensity=(0.1, 1), color_shift=(0.01, 0.05)),\n",
    "                albumentations.ImageCompression(\n",
    "                    quality_lower=70, quality_upper=100, compression_type=1\n",
    "                ),\n",
    "                albumentations.MultiplicativeNoise(\n",
    "                    multiplier=(0.95, 1.05), per_channel=True, elementwise=True\n",
    "                ),\n",
    "                albumentations.Downscale(\n",
    "                    scale_min=0.6, scale_max=0.99, interpolation=4\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.HueSaturationValue(\n",
    "                    hue_shift_limit=(-7, 7),\n",
    "                    sat_shift_limit=(-10, 10),\n",
    "                    val_shift_limit=(-10, 10),\n",
    "                ),\n",
    "                albumentations.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.15,\n",
    "                    contrast_limit=0.2,\n",
    "                    brightness_by_max=True,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(\n",
    "                    distort_limit=0.05,\n",
    "                    shift_limit=0.05,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "                albumentations.ElasticTransform(\n",
    "                    alpha=2.0,\n",
    "                    sigma=50.0,\n",
    "                    alpha_affine=10.0,\n",
    "                    interpolation=0,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "                albumentations.GridDistortion(\n",
    "                    num_steps=5, distort_limit=0.3, interpolation=0, border_mode=2\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.HorizontalFlip(),\n",
    "                albumentations.VerticalFlip(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.Rotate(\n",
    "                    limit=(-180, 180), interpolation=0, border_mode=2\n",
    "                ),\n",
    "                albumentations.ShiftScaleRotate(\n",
    "                    shift_limit=0.05,\n",
    "                    scale_limit=0.05,\n",
    "                    rotate_limit=180,\n",
    "                    interpolation=0,\n",
    "                    border_mode=2,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        images_src_dir,\n",
    "        batch_size,\n",
    "        target_image_size,\n",
    "        shuffle=False,\n",
    "        augment=True,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "        normalize=False,\n",
    "    ):\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.df = df\n",
    "        self.images_dir = images_src_dir\n",
    "        self.target_image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "        self.augment = augment\n",
    "        self.crop = crop\n",
    "        self.resize = resize\n",
    "        self.normalize = normalize\n",
    "        # create label index map\n",
    "        self.labels = self._read_labels()\n",
    "        self.n_samples = self.df.shape[0]\n",
    "        self.n_batches = self.n_samples // self.batch_size\n",
    "        # shuffle data, also repeated after each epoch if needed\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.labels)\n",
    "\n",
    "    def _read_labels(self):\n",
    "        \"\"\"\n",
    "        Returns list images mapping to 1-hot label\n",
    "        \"\"\"\n",
    "\n",
    "        # label indexes\n",
    "        label_ixs = self.df[feature_columns].values\n",
    "        image_ixs = self.df[\"image\"].values\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(image_ixs)):\n",
    "            labels.append([image_ixs[i], label_ixs[i]])\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length in batches\n",
    "        \"\"\"\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, b_ix):\n",
    "        \"\"\"\n",
    "        Produce batch, by batch index\n",
    "        \"\"\"\n",
    "\n",
    "        assert b_ix < self.n_batches\n",
    "\n",
    "        b_X = np.zeros(\n",
    "            (self.batch_size, self.target_image_size[0], self.target_image_size[1], 3),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        b_Y = np.zeros(\n",
    "            (self.batch_size, self.df[feature_columns].shape[1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            b_X[i], b_Y[i] = self.get_one(\n",
    "                i + self.batch_size * b_ix,\n",
    "            )\n",
    "\n",
    "        return (b_X, b_Y)\n",
    "\n",
    "    def get_one(self, one_ix):\n",
    "        \"\"\"\n",
    "        Get single item by absolute index\n",
    "        \"\"\"\n",
    "        id = self.labels[one_ix][0]\n",
    "        src_file = self.images_dir + id\n",
    "\n",
    "        # read file\n",
    "        x = np.load(src_file)\n",
    "        if self.crop:\n",
    "            coord = self.df[self.df[\"image\"] == id][\n",
    "                [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]\n",
    "            ].values[0]\n",
    "            orig_hight = x.shape[0]\n",
    "            orig_width = x.shape[1]\n",
    "            x_min = coord[0]\n",
    "            y_min = coord[1]\n",
    "            x_max = coord[2]\n",
    "            y_max = coord[3]\n",
    "            x = x[\n",
    "                np.int(y_min * orig_hight) : np.int(y_max * orig_hight),\n",
    "                np.int(x_min * orig_width) : np.int(x_max * orig_width),\n",
    "            ]\n",
    "\n",
    "        y = self.labels[one_ix][1]\n",
    "\n",
    "        # augment\n",
    "        if self.augment:\n",
    "            x = self._augment_image(x)\n",
    "\n",
    "        # normalize (sample-wise)\n",
    "        if self.normalize:\n",
    "            x = x.astype(np.float32)\n",
    "            x = x - np.mean(x, axis=(0, 1))\n",
    "            x = x / np.std(x, axis=(0, 1))\n",
    "        return x.astype(np.uint8), y\n",
    "\n",
    "    def _augment_image(self, x):\n",
    "        \"\"\"\n",
    "        Randomply augment image\n",
    "        \"\"\"\n",
    "\n",
    "        x = transform(image=x)[\"image\"]\n",
    "        return x\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "policy = keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f1_sample(y_true, y_pred):\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)[0]\n",
    "#     f1_score = sklearn.metrics.f1_score(y_true, y_pred, average='samples')\n",
    "#     return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.mean(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.mean(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.mean(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.mean(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def f1_sample(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    base_model = keras.applications.EfficientNetB4(weights=None, include_top=False)\n",
    "    base_model.load_weights(\n",
    "        \"/app/_data/models/efficientnet-b4_noisy-student_notop.h5\",\n",
    "        by_name=True,\n",
    "        skip_mismatch=True,\n",
    "    )\n",
    "    x = base_model(inputs)\n",
    "    x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "    x = keras.layers.Flatten(name=\"flatten\")(x)\n",
    "    outputs = keras.layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(lr=0.0005),\n",
    "        metrics=[\n",
    "            \"acc\",\n",
    "            tfa.metrics.F1Score(num_classes=NUM_CLASSES, average=\"weighted\"),\n",
    "            f1_sample,\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Metrics(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.f1s = []\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "# #         score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "#         predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "#         targ = self.validation_data[1]\n",
    "\n",
    "#         self.f1s.append(sklearn.metrics.f1_score(targ, predict, average='samples'))\n",
    "\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/288 [..............................] - ETA: 0s - loss: 0.6875 - acc: 0.2500 - f1_score: 0.2389 - f1_sample: 0.2977WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1466 - acc: 0.8438 - f1_score: 0.8314 - f1_sample: 0.8337\n",
      "Epoch 00001: val_f1_sample improved from -inf to 0.86984, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 129s 450ms/step - loss: 0.1466 - acc: 0.8438 - f1_score: 0.8314 - f1_sample: 0.8337 - val_loss: 0.1255 - val_acc: 0.8832 - val_f1_score: 0.8668 - val_f1_sample: 0.8698\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0993 - acc: 0.8956 - f1_score: 0.8827 - f1_sample: 0.8903\n",
      "Epoch 00002: val_f1_sample improved from 0.86984 to 0.90006, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0993 - acc: 0.8956 - f1_score: 0.8827 - f1_sample: 0.8903 - val_loss: 0.0882 - val_acc: 0.8968 - val_f1_score: 0.8777 - val_f1_sample: 0.9001\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0887 - acc: 0.9030 - f1_score: 0.8903 - f1_sample: 0.8980\n",
      "Epoch 00003: val_f1_sample improved from 0.90006 to 0.90022, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 134s 467ms/step - loss: 0.0887 - acc: 0.9030 - f1_score: 0.8903 - f1_sample: 0.8980 - val_loss: 0.0897 - val_acc: 0.8956 - val_f1_score: 0.8884 - val_f1_sample: 0.9002\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0810 - acc: 0.9092 - f1_score: 0.8974 - f1_sample: 0.9079\n",
      "Epoch 00004: val_f1_sample did not improve from 0.90022\n",
      "288/288 [==============================] - 129s 448ms/step - loss: 0.0810 - acc: 0.9092 - f1_score: 0.8974 - f1_sample: 0.9079 - val_loss: 0.1022 - val_acc: 0.9023 - val_f1_score: 0.8834 - val_f1_sample: 0.8845\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0750 - acc: 0.9151 - f1_score: 0.9031 - f1_sample: 0.9134\n",
      "Epoch 00005: val_f1_sample improved from 0.90022 to 0.91301, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0750 - acc: 0.9151 - f1_score: 0.9031 - f1_sample: 0.9134 - val_loss: 0.0770 - val_acc: 0.9010 - val_f1_score: 0.8977 - val_f1_sample: 0.9130\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0711 - acc: 0.9214 - f1_score: 0.9101 - f1_sample: 0.9201\n",
      "Epoch 00006: val_f1_sample did not improve from 0.91301\n",
      "288/288 [==============================] - 130s 453ms/step - loss: 0.0711 - acc: 0.9214 - f1_score: 0.9101 - f1_sample: 0.9201 - val_loss: 0.0980 - val_acc: 0.9028 - val_f1_score: 0.8872 - val_f1_sample: 0.8973\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0676 - acc: 0.9221 - f1_score: 0.9108 - f1_sample: 0.9227\n",
      "Epoch 00007: val_f1_sample improved from 0.91301 to 0.91438, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0676 - acc: 0.9221 - f1_score: 0.9108 - f1_sample: 0.9227 - val_loss: 0.0788 - val_acc: 0.9204 - val_f1_score: 0.9063 - val_f1_sample: 0.9144\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0635 - acc: 0.9262 - f1_score: 0.9149 - f1_sample: 0.9291\n",
      "Epoch 00008: val_f1_sample did not improve from 0.91438\n",
      "288/288 [==============================] - 131s 454ms/step - loss: 0.0635 - acc: 0.9262 - f1_score: 0.9149 - f1_sample: 0.9291 - val_loss: 0.0854 - val_acc: 0.9187 - val_f1_score: 0.9043 - val_f1_sample: 0.9131\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0634 - acc: 0.9266 - f1_score: 0.9162 - f1_sample: 0.9294\n",
      "Epoch 00009: val_f1_sample did not improve from 0.91438\n",
      "288/288 [==============================] - 129s 449ms/step - loss: 0.0634 - acc: 0.9266 - f1_score: 0.9162 - f1_sample: 0.9294 - val_loss: 0.0966 - val_acc: 0.9102 - val_f1_score: 0.8972 - val_f1_sample: 0.9089\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0584 - acc: 0.9292 - f1_score: 0.9208 - f1_sample: 0.9347\n",
      "Epoch 00010: val_f1_sample did not improve from 0.91438\n",
      "288/288 [==============================] - 132s 458ms/step - loss: 0.0584 - acc: 0.9292 - f1_score: 0.9208 - f1_sample: 0.9347 - val_loss: 0.2464 - val_acc: 0.8842 - val_f1_score: 0.8674 - val_f1_sample: 0.8754\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0540 - acc: 0.9350 - f1_score: 0.9244 - f1_sample: 0.9399\n",
      "Epoch 00011: val_f1_sample improved from 0.91438 to 0.92426, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 132s 458ms/step - loss: 0.0540 - acc: 0.9350 - f1_score: 0.9244 - f1_sample: 0.9399 - val_loss: 0.0705 - val_acc: 0.9286 - val_f1_score: 0.9135 - val_f1_sample: 0.9243\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0519 - acc: 0.9368 - f1_score: 0.9273 - f1_sample: 0.9420\n",
      "Epoch 00012: val_f1_sample did not improve from 0.92426\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0519 - acc: 0.9368 - f1_score: 0.9273 - f1_sample: 0.9420 - val_loss: 0.0769 - val_acc: 0.9253 - val_f1_score: 0.9092 - val_f1_sample: 0.9195\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9342 - f1_score: 0.9260 - f1_sample: 0.9437\n",
      "Epoch 00013: val_f1_sample did not improve from 0.92426\n",
      "288/288 [==============================] - 129s 448ms/step - loss: 0.0504 - acc: 0.9342 - f1_score: 0.9260 - f1_sample: 0.9437 - val_loss: 0.0796 - val_acc: 0.9266 - val_f1_score: 0.9095 - val_f1_sample: 0.9183\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0476 - acc: 0.9408 - f1_score: 0.9306 - f1_sample: 0.9462\n",
      "Epoch 00014: val_f1_sample did not improve from 0.92426\n",
      "288/288 [==============================] - 129s 446ms/step - loss: 0.0476 - acc: 0.9408 - f1_score: 0.9306 - f1_sample: 0.9462 - val_loss: 0.0770 - val_acc: 0.9224 - val_f1_score: 0.9069 - val_f1_sample: 0.9182\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0479 - acc: 0.9430 - f1_score: 0.9306 - f1_sample: 0.9466\n",
      "Epoch 00015: val_f1_sample did not improve from 0.92426\n",
      "288/288 [==============================] - 128s 444ms/step - loss: 0.0479 - acc: 0.9430 - f1_score: 0.9306 - f1_sample: 0.9466 - val_loss: 0.0828 - val_acc: 0.9182 - val_f1_score: 0.9026 - val_f1_sample: 0.9176\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0426 - acc: 0.9443 - f1_score: 0.9359 - f1_sample: 0.9540\n",
      "Epoch 00016: val_f1_sample did not improve from 0.92426\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0426 - acc: 0.9443 - f1_score: 0.9359 - f1_sample: 0.9540 - val_loss: 0.0870 - val_acc: 0.9125 - val_f1_score: 0.9019 - val_f1_sample: 0.9133\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0389 - acc: 0.9435 - f1_score: 0.9362 - f1_sample: 0.9562\n",
      "Epoch 00017: val_f1_sample did not improve from 0.92426\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "288/288 [==============================] - 128s 443ms/step - loss: 0.0389 - acc: 0.9435 - f1_score: 0.9362 - f1_sample: 0.9562 - val_loss: 0.0849 - val_acc: 0.9132 - val_f1_score: 0.9054 - val_f1_sample: 0.9139\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9477 - f1_score: 0.9422 - f1_sample: 0.9634\n",
      "Epoch 00018: val_f1_sample improved from 0.92426 to 0.92529, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_1.h5\n",
      "288/288 [==============================] - 132s 457ms/step - loss: 0.0329 - acc: 0.9477 - f1_score: 0.9422 - f1_sample: 0.9634 - val_loss: 0.0794 - val_acc: 0.9177 - val_f1_score: 0.9122 - val_f1_sample: 0.9253\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0317 - acc: 0.9498 - f1_score: 0.9433 - f1_sample: 0.9656\n",
      "Epoch 00019: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0317 - acc: 0.9498 - f1_score: 0.9433 - f1_sample: 0.9656 - val_loss: 0.0835 - val_acc: 0.9221 - val_f1_score: 0.9068 - val_f1_sample: 0.9229\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0291 - acc: 0.9564 - f1_score: 0.9463 - f1_sample: 0.9688\n",
      "Epoch 00020: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 129s 446ms/step - loss: 0.0291 - acc: 0.9564 - f1_score: 0.9463 - f1_sample: 0.9688 - val_loss: 0.0856 - val_acc: 0.9191 - val_f1_score: 0.9080 - val_f1_sample: 0.9240\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0276 - acc: 0.9534 - f1_score: 0.9468 - f1_sample: 0.9694\n",
      "Epoch 00021: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0276 - acc: 0.9534 - f1_score: 0.9468 - f1_sample: 0.9694 - val_loss: 0.0813 - val_acc: 0.9182 - val_f1_score: 0.9076 - val_f1_sample: 0.9207\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0245 - acc: 0.9572 - f1_score: 0.9494 - f1_sample: 0.9737\n",
      "Epoch 00022: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0245 - acc: 0.9572 - f1_score: 0.9494 - f1_sample: 0.9737 - val_loss: 0.1074 - val_acc: 0.9219 - val_f1_score: 0.9103 - val_f1_sample: 0.9184\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9583 - f1_score: 0.9513 - f1_sample: 0.9743\n",
      "Epoch 00023: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 130s 452ms/step - loss: 0.0244 - acc: 0.9583 - f1_score: 0.9513 - f1_sample: 0.9743 - val_loss: 0.0980 - val_acc: 0.9139 - val_f1_score: 0.9086 - val_f1_sample: 0.9221\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.9550 - f1_score: 0.9509 - f1_sample: 0.9747\n",
      "Epoch 00024: val_f1_sample did not improve from 0.92529\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "288/288 [==============================] - 129s 446ms/step - loss: 0.0239 - acc: 0.9550 - f1_score: 0.9509 - f1_sample: 0.9747 - val_loss: 0.0886 - val_acc: 0.9221 - val_f1_score: 0.9127 - val_f1_sample: 0.9212\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0187 - acc: 0.9609 - f1_score: 0.9537 - f1_sample: 0.9804\n",
      "Epoch 00025: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 129s 449ms/step - loss: 0.0187 - acc: 0.9609 - f1_score: 0.9537 - f1_sample: 0.9804 - val_loss: 0.1136 - val_acc: 0.9244 - val_f1_score: 0.9097 - val_f1_sample: 0.9217\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0187 - acc: 0.9626 - f1_score: 0.9557 - f1_sample: 0.9798Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00026: val_f1_sample did not improve from 0.92529\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0187 - acc: 0.9626 - f1_score: 0.9557 - f1_sample: 0.9798 - val_loss: 0.0959 - val_acc: 0.9281 - val_f1_score: 0.9136 - val_f1_sample: 0.9230\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/100\n",
      "  2/288 [..............................] - ETA: 7:16 - loss: 0.6750 - acc: 0.2589 - f1_score: 0.2559 - f1_sample: 0.2756WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.8221s vs `on_train_batch_end` time: 2.2305s). Check your callbacks.\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1481 - acc: 0.8442 - f1_score: 0.8315 - f1_sample: 0.8291\n",
      "Epoch 00001: val_f1_sample improved from -inf to 0.86064, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 135s 468ms/step - loss: 0.1481 - acc: 0.8442 - f1_score: 0.8315 - f1_sample: 0.8291 - val_loss: 0.1230 - val_acc: 0.8666 - val_f1_score: 0.8553 - val_f1_sample: 0.8606\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0962 - acc: 0.8961 - f1_score: 0.8843 - f1_sample: 0.8918\n",
      "Epoch 00002: val_f1_sample improved from 0.86064 to 0.89089, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0962 - acc: 0.8961 - f1_score: 0.8843 - f1_sample: 0.8918 - val_loss: 0.0964 - val_acc: 0.9092 - val_f1_score: 0.8878 - val_f1_sample: 0.8909\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0881 - acc: 0.9028 - f1_score: 0.8905 - f1_sample: 0.9004\n",
      "Epoch 00003: val_f1_sample improved from 0.89089 to 0.89626, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 133s 462ms/step - loss: 0.0881 - acc: 0.9028 - f1_score: 0.8905 - f1_sample: 0.9004 - val_loss: 0.0908 - val_acc: 0.9050 - val_f1_score: 0.8909 - val_f1_sample: 0.8963\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0779 - acc: 0.9137 - f1_score: 0.9028 - f1_sample: 0.9119\n",
      "Epoch 00004: val_f1_sample improved from 0.89626 to 0.90637, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0779 - acc: 0.9137 - f1_score: 0.9028 - f1_sample: 0.9119 - val_loss: 0.0881 - val_acc: 0.9082 - val_f1_score: 0.8924 - val_f1_sample: 0.9064\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0730 - acc: 0.9175 - f1_score: 0.9064 - f1_sample: 0.9177\n",
      "Epoch 00005: val_f1_sample did not improve from 0.90637\n",
      "288/288 [==============================] - 131s 453ms/step - loss: 0.0730 - acc: 0.9175 - f1_score: 0.9064 - f1_sample: 0.9177 - val_loss: 0.0865 - val_acc: 0.9065 - val_f1_score: 0.8912 - val_f1_sample: 0.9042\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0706 - acc: 0.9195 - f1_score: 0.9079 - f1_sample: 0.9212\n",
      "Epoch 00006: val_f1_sample did not improve from 0.90637\n",
      "288/288 [==============================] - 129s 447ms/step - loss: 0.0706 - acc: 0.9195 - f1_score: 0.9079 - f1_sample: 0.9212 - val_loss: 0.0874 - val_acc: 0.9018 - val_f1_score: 0.8935 - val_f1_sample: 0.9017\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0660 - acc: 0.9252 - f1_score: 0.9137 - f1_sample: 0.9268\n",
      "Epoch 00007: val_f1_sample did not improve from 0.90637\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0660 - acc: 0.9252 - f1_score: 0.9137 - f1_sample: 0.9268 - val_loss: 0.0854 - val_acc: 0.8971 - val_f1_score: 0.8912 - val_f1_sample: 0.9003\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0619 - acc: 0.9263 - f1_score: 0.9164 - f1_sample: 0.9326\n",
      "Epoch 00008: val_f1_sample improved from 0.90637 to 0.91367, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 129s 450ms/step - loss: 0.0619 - acc: 0.9263 - f1_score: 0.9164 - f1_sample: 0.9326 - val_loss: 0.0828 - val_acc: 0.9184 - val_f1_score: 0.9027 - val_f1_sample: 0.9137\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0583 - acc: 0.9311 - f1_score: 0.9210 - f1_sample: 0.9352\n",
      "Epoch 00009: val_f1_sample did not improve from 0.91367\n",
      "288/288 [==============================] - 124s 431ms/step - loss: 0.0583 - acc: 0.9311 - f1_score: 0.9210 - f1_sample: 0.9352 - val_loss: 0.1211 - val_acc: 0.8829 - val_f1_score: 0.8726 - val_f1_sample: 0.8816\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0576 - acc: 0.9305 - f1_score: 0.9207 - f1_sample: 0.9357\n",
      "Epoch 00010: val_f1_sample did not improve from 0.91367\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0576 - acc: 0.9305 - f1_score: 0.9207 - f1_sample: 0.9357 - val_loss: 0.0829 - val_acc: 0.9182 - val_f1_score: 0.9041 - val_f1_sample: 0.9126\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0544 - acc: 0.9344 - f1_score: 0.9234 - f1_sample: 0.9393\n",
      "Epoch 00011: val_f1_sample did not improve from 0.91367\n",
      "288/288 [==============================] - 126s 437ms/step - loss: 0.0544 - acc: 0.9344 - f1_score: 0.9234 - f1_sample: 0.9393 - val_loss: 0.1134 - val_acc: 0.9107 - val_f1_score: 0.8933 - val_f1_sample: 0.8983\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0502 - acc: 0.9384 - f1_score: 0.9287 - f1_sample: 0.9447\n",
      "Epoch 00012: val_f1_sample did not improve from 0.91367\n",
      "288/288 [==============================] - 128s 444ms/step - loss: 0.0502 - acc: 0.9384 - f1_score: 0.9287 - f1_sample: 0.9447 - val_loss: 0.1075 - val_acc: 0.9058 - val_f1_score: 0.8929 - val_f1_sample: 0.9003\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0488 - acc: 0.9395 - f1_score: 0.9307 - f1_sample: 0.9459\n",
      "Epoch 00013: val_f1_sample did not improve from 0.91367\n",
      "288/288 [==============================] - 124s 431ms/step - loss: 0.0488 - acc: 0.9395 - f1_score: 0.9307 - f1_sample: 0.9459 - val_loss: 0.0942 - val_acc: 0.9137 - val_f1_score: 0.9006 - val_f1_sample: 0.9105\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.9408 - f1_score: 0.9318 - f1_sample: 0.9503\n",
      "Epoch 00014: val_f1_sample did not improve from 0.91367\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "288/288 [==============================] - 128s 446ms/step - loss: 0.0459 - acc: 0.9408 - f1_score: 0.9318 - f1_sample: 0.9503 - val_loss: 0.0997 - val_acc: 0.9125 - val_f1_score: 0.8945 - val_f1_sample: 0.8993\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0391 - acc: 0.9461 - f1_score: 0.9369 - f1_sample: 0.9574\n",
      "Epoch 00015: val_f1_sample improved from 0.91367 to 0.91774, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 129s 447ms/step - loss: 0.0391 - acc: 0.9461 - f1_score: 0.9369 - f1_sample: 0.9574 - val_loss: 0.0918 - val_acc: 0.9189 - val_f1_score: 0.9060 - val_f1_sample: 0.9177\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0368 - acc: 0.9503 - f1_score: 0.9409 - f1_sample: 0.9609\n",
      "Epoch 00016: val_f1_sample improved from 0.91774 to 0.91971, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 128s 443ms/step - loss: 0.0368 - acc: 0.9503 - f1_score: 0.9409 - f1_sample: 0.9609 - val_loss: 0.0906 - val_acc: 0.9219 - val_f1_score: 0.9078 - val_f1_sample: 0.9197\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0345 - acc: 0.9537 - f1_score: 0.9414 - f1_sample: 0.9617\n",
      "Epoch 00017: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0345 - acc: 0.9537 - f1_score: 0.9414 - f1_sample: 0.9617 - val_loss: 0.1140 - val_acc: 0.9147 - val_f1_score: 0.9004 - val_f1_sample: 0.9112\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0325 - acc: 0.9531 - f1_score: 0.9447 - f1_sample: 0.9655\n",
      "Epoch 00018: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0325 - acc: 0.9531 - f1_score: 0.9447 - f1_sample: 0.9655 - val_loss: 0.0960 - val_acc: 0.9224 - val_f1_score: 0.9081 - val_f1_sample: 0.9174\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0314 - acc: 0.9535 - f1_score: 0.9438 - f1_sample: 0.9658\n",
      "Epoch 00019: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 125s 434ms/step - loss: 0.0314 - acc: 0.9535 - f1_score: 0.9438 - f1_sample: 0.9658 - val_loss: 0.1050 - val_acc: 0.9201 - val_f1_score: 0.9043 - val_f1_sample: 0.9161\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0288 - acc: 0.9555 - f1_score: 0.9471 - f1_sample: 0.9694\n",
      "Epoch 00020: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 127s 443ms/step - loss: 0.0288 - acc: 0.9555 - f1_score: 0.9471 - f1_sample: 0.9694 - val_loss: 0.1056 - val_acc: 0.9132 - val_f1_score: 0.9044 - val_f1_sample: 0.9151\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0263 - acc: 0.9555 - f1_score: 0.9484 - f1_sample: 0.9707\n",
      "Epoch 00021: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0263 - acc: 0.9555 - f1_score: 0.9484 - f1_sample: 0.9707 - val_loss: 0.1058 - val_acc: 0.9157 - val_f1_score: 0.9031 - val_f1_sample: 0.9172\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9525 - f1_score: 0.9480 - f1_sample: 0.9694\n",
      "Epoch 00022: val_f1_sample did not improve from 0.91971\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "288/288 [==============================] - 127s 440ms/step - loss: 0.0277 - acc: 0.9525 - f1_score: 0.9480 - f1_sample: 0.9694 - val_loss: 0.1059 - val_acc: 0.9162 - val_f1_score: 0.9012 - val_f1_sample: 0.9115\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0196 - acc: 0.9597 - f1_score: 0.9536 - f1_sample: 0.9788\n",
      "Epoch 00023: val_f1_sample did not improve from 0.91971\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0196 - acc: 0.9597 - f1_score: 0.9536 - f1_sample: 0.9788 - val_loss: 0.1011 - val_acc: 0.9266 - val_f1_score: 0.9097 - val_f1_sample: 0.9191\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9611 - f1_score: 0.9531 - f1_sample: 0.9785\n",
      "Epoch 00024: val_f1_sample improved from 0.91971 to 0.92281, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0199 - acc: 0.9611 - f1_score: 0.9531 - f1_sample: 0.9785 - val_loss: 0.1096 - val_acc: 0.9261 - val_f1_score: 0.9131 - val_f1_sample: 0.9228\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9611 - f1_score: 0.9546 - f1_sample: 0.9790\n",
      "Epoch 00025: val_f1_sample did not improve from 0.92281\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0199 - acc: 0.9611 - f1_score: 0.9546 - f1_sample: 0.9790 - val_loss: 0.1017 - val_acc: 0.9246 - val_f1_score: 0.9104 - val_f1_sample: 0.9174\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9619 - f1_score: 0.9566 - f1_sample: 0.9827\n",
      "Epoch 00026: val_f1_sample did not improve from 0.92281\n",
      "288/288 [==============================] - 126s 438ms/step - loss: 0.0169 - acc: 0.9619 - f1_score: 0.9566 - f1_sample: 0.9827 - val_loss: 0.1157 - val_acc: 0.9214 - val_f1_score: 0.9089 - val_f1_sample: 0.9170\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0187 - acc: 0.9567 - f1_score: 0.9542 - f1_sample: 0.9807\n",
      "Epoch 00027: val_f1_sample did not improve from 0.92281\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0187 - acc: 0.9567 - f1_score: 0.9542 - f1_sample: 0.9807 - val_loss: 0.1252 - val_acc: 0.9214 - val_f1_score: 0.9061 - val_f1_sample: 0.9186\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9628 - f1_score: 0.9585 - f1_sample: 0.9830\n",
      "Epoch 00028: val_f1_sample did not improve from 0.92281\n",
      "288/288 [==============================] - 126s 436ms/step - loss: 0.0161 - acc: 0.9628 - f1_score: 0.9585 - f1_sample: 0.9830 - val_loss: 0.1296 - val_acc: 0.9199 - val_f1_score: 0.9087 - val_f1_sample: 0.9177\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.9635 - f1_score: 0.9591 - f1_sample: 0.9831\n",
      "Epoch 00029: val_f1_sample did not improve from 0.92281\n",
      "288/288 [==============================] - 128s 443ms/step - loss: 0.0152 - acc: 0.9635 - f1_score: 0.9591 - f1_sample: 0.9831 - val_loss: 0.1225 - val_acc: 0.9090 - val_f1_score: 0.9050 - val_f1_sample: 0.9130\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9629 - f1_score: 0.9570 - f1_sample: 0.9827\n",
      "Epoch 00030: val_f1_sample did not improve from 0.92281\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0002560000168159604.\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0170 - acc: 0.9629 - f1_score: 0.9570 - f1_sample: 0.9827 - val_loss: 0.1351 - val_acc: 0.9194 - val_f1_score: 0.9063 - val_f1_sample: 0.9155\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9640 - f1_score: 0.9591 - f1_sample: 0.9852\n",
      "Epoch 00031: val_f1_sample improved from 0.92281 to 0.92438, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_2.h5\n",
      "288/288 [==============================] - 127s 440ms/step - loss: 0.0141 - acc: 0.9640 - f1_score: 0.9591 - f1_sample: 0.9852 - val_loss: 0.1153 - val_acc: 0.9241 - val_f1_score: 0.9137 - val_f1_sample: 0.9244\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.9639 - f1_score: 0.9634 - f1_sample: 0.9890\n",
      "Epoch 00032: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0110 - acc: 0.9639 - f1_score: 0.9634 - f1_sample: 0.9890 - val_loss: 0.1266 - val_acc: 0.9209 - val_f1_score: 0.9097 - val_f1_sample: 0.9179\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0107 - acc: 0.9639 - f1_score: 0.9645 - f1_sample: 0.9888\n",
      "Epoch 00033: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 125s 433ms/step - loss: 0.0107 - acc: 0.9639 - f1_score: 0.9645 - f1_sample: 0.9888 - val_loss: 0.1257 - val_acc: 0.9194 - val_f1_score: 0.9116 - val_f1_sample: 0.9221\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9636 - f1_score: 0.9625 - f1_sample: 0.9883\n",
      "Epoch 00034: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 131s 453ms/step - loss: 0.0111 - acc: 0.9636 - f1_score: 0.9625 - f1_sample: 0.9883 - val_loss: 0.1295 - val_acc: 0.9177 - val_f1_score: 0.9062 - val_f1_sample: 0.9151\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.9679 - f1_score: 0.9637 - f1_sample: 0.9879\n",
      "Epoch 00035: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 131s 453ms/step - loss: 0.0110 - acc: 0.9679 - f1_score: 0.9637 - f1_sample: 0.9879 - val_loss: 0.1293 - val_acc: 0.9204 - val_f1_score: 0.9071 - val_f1_sample: 0.9163\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9647 - f1_score: 0.9619 - f1_sample: 0.9879\n",
      "Epoch 00036: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 128s 444ms/step - loss: 0.0113 - acc: 0.9647 - f1_score: 0.9619 - f1_sample: 0.9879 - val_loss: 0.1267 - val_acc: 0.9226 - val_f1_score: 0.9106 - val_f1_sample: 0.9220\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9671 - f1_score: 0.9661 - f1_sample: 0.9893\n",
      "Epoch 00037: val_f1_sample did not improve from 0.92438\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00020480002276599408.\n",
      "288/288 [==============================] - 129s 448ms/step - loss: 0.0101 - acc: 0.9671 - f1_score: 0.9661 - f1_sample: 0.9893 - val_loss: 0.1461 - val_acc: 0.9201 - val_f1_score: 0.9112 - val_f1_sample: 0.9215\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9669 - f1_score: 0.9678 - f1_sample: 0.9900\n",
      "Epoch 00038: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 130s 453ms/step - loss: 0.0096 - acc: 0.9669 - f1_score: 0.9678 - f1_sample: 0.9900 - val_loss: 0.1456 - val_acc: 0.9256 - val_f1_score: 0.9108 - val_f1_sample: 0.9187\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9663 - f1_score: 0.9686 - f1_sample: 0.9915Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00039: val_f1_sample did not improve from 0.92438\n",
      "288/288 [==============================] - 130s 452ms/step - loss: 0.0081 - acc: 0.9663 - f1_score: 0.9686 - f1_sample: 0.9915 - val_loss: 0.1390 - val_acc: 0.9226 - val_f1_score: 0.9103 - val_f1_sample: 0.9191\n",
      "Epoch 00039: early stopping\n",
      "Epoch 1/100\n",
      "  2/288 [..............................] - ETA: 6:20 - loss: 0.6729 - acc: 0.1607 - f1_score: 0.1727 - f1_sample: 0.2800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3836s vs `on_train_batch_end` time: 2.2761s). Check your callbacks.\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1490 - acc: 0.8385 - f1_score: 0.8266 - f1_sample: 0.8245\n",
      "Epoch 00001: val_f1_sample improved from -inf to 0.87509, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 135s 468ms/step - loss: 0.1490 - acc: 0.8385 - f1_score: 0.8266 - f1_sample: 0.8245 - val_loss: 0.1153 - val_acc: 0.8824 - val_f1_score: 0.8663 - val_f1_sample: 0.8751\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0988 - acc: 0.8937 - f1_score: 0.8819 - f1_sample: 0.8890\n",
      "Epoch 00002: val_f1_sample improved from 0.87509 to 0.90834, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 131s 453ms/step - loss: 0.0988 - acc: 0.8937 - f1_score: 0.8819 - f1_sample: 0.8890 - val_loss: 0.0840 - val_acc: 0.9125 - val_f1_score: 0.8986 - val_f1_sample: 0.9083\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0874 - acc: 0.9058 - f1_score: 0.8933 - f1_sample: 0.9016\n",
      "Epoch 00003: val_f1_sample did not improve from 0.90834\n",
      "288/288 [==============================] - 127s 440ms/step - loss: 0.0874 - acc: 0.9058 - f1_score: 0.8933 - f1_sample: 0.9016 - val_loss: 0.0887 - val_acc: 0.9154 - val_f1_score: 0.8952 - val_f1_sample: 0.9059\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0791 - acc: 0.9139 - f1_score: 0.9015 - f1_sample: 0.9117\n",
      "Epoch 00004: val_f1_sample did not improve from 0.90834\n",
      "288/288 [==============================] - 131s 457ms/step - loss: 0.0791 - acc: 0.9139 - f1_score: 0.9015 - f1_sample: 0.9117 - val_loss: 0.0870 - val_acc: 0.9122 - val_f1_score: 0.8939 - val_f1_sample: 0.9052\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0765 - acc: 0.9154 - f1_score: 0.9038 - f1_sample: 0.9141\n",
      "Epoch 00005: val_f1_sample improved from 0.90834 to 0.91369, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 132s 458ms/step - loss: 0.0765 - acc: 0.9154 - f1_score: 0.9038 - f1_sample: 0.9141 - val_loss: 0.0778 - val_acc: 0.9182 - val_f1_score: 0.9000 - val_f1_sample: 0.9137\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0713 - acc: 0.9195 - f1_score: 0.9080 - f1_sample: 0.9202\n",
      "Epoch 00006: val_f1_sample did not improve from 0.91369\n",
      "288/288 [==============================] - 129s 447ms/step - loss: 0.0713 - acc: 0.9195 - f1_score: 0.9080 - f1_sample: 0.9202 - val_loss: 0.1066 - val_acc: 0.8896 - val_f1_score: 0.8664 - val_f1_sample: 0.8906\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0660 - acc: 0.9249 - f1_score: 0.9142 - f1_sample: 0.9267\n",
      "Epoch 00007: val_f1_sample did not improve from 0.91369\n",
      "288/288 [==============================] - 128s 444ms/step - loss: 0.0660 - acc: 0.9249 - f1_score: 0.9142 - f1_sample: 0.9267 - val_loss: 0.0917 - val_acc: 0.9149 - val_f1_score: 0.8921 - val_f1_sample: 0.8979\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0638 - acc: 0.9260 - f1_score: 0.9152 - f1_sample: 0.9282\n",
      "Epoch 00008: val_f1_sample did not improve from 0.91369\n",
      "288/288 [==============================] - 127s 443ms/step - loss: 0.0638 - acc: 0.9260 - f1_score: 0.9152 - f1_sample: 0.9282 - val_loss: 0.0849 - val_acc: 0.9219 - val_f1_score: 0.9018 - val_f1_sample: 0.9098\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0625 - acc: 0.9299 - f1_score: 0.9184 - f1_sample: 0.9321\n",
      "Epoch 00009: val_f1_sample did not improve from 0.91369\n",
      "288/288 [==============================] - 130s 452ms/step - loss: 0.0625 - acc: 0.9299 - f1_score: 0.9184 - f1_sample: 0.9321 - val_loss: 0.0927 - val_acc: 0.9050 - val_f1_score: 0.8915 - val_f1_sample: 0.9104\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0577 - acc: 0.9337 - f1_score: 0.9218 - f1_sample: 0.9362\n",
      "Epoch 00010: val_f1_sample improved from 0.91369 to 0.91463, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0577 - acc: 0.9337 - f1_score: 0.9218 - f1_sample: 0.9362 - val_loss: 0.0864 - val_acc: 0.8998 - val_f1_score: 0.8964 - val_f1_sample: 0.9146\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0541 - acc: 0.9357 - f1_score: 0.9252 - f1_sample: 0.9399\n",
      "Epoch 00011: val_f1_sample did not improve from 0.91463\n",
      "288/288 [==============================] - 131s 454ms/step - loss: 0.0541 - acc: 0.9357 - f1_score: 0.9252 - f1_sample: 0.9399 - val_loss: 0.0851 - val_acc: 0.9125 - val_f1_score: 0.9011 - val_f1_sample: 0.9130\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0507 - acc: 0.9397 - f1_score: 0.9295 - f1_sample: 0.9443\n",
      "Epoch 00012: val_f1_sample improved from 0.91463 to 0.91563, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 132s 457ms/step - loss: 0.0507 - acc: 0.9397 - f1_score: 0.9295 - f1_sample: 0.9443 - val_loss: 0.0841 - val_acc: 0.9234 - val_f1_score: 0.9047 - val_f1_sample: 0.9156\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0471 - acc: 0.9427 - f1_score: 0.9313 - f1_sample: 0.9471\n",
      "Epoch 00013: val_f1_sample did not improve from 0.91563\n",
      "288/288 [==============================] - 128s 446ms/step - loss: 0.0471 - acc: 0.9427 - f1_score: 0.9313 - f1_sample: 0.9471 - val_loss: 0.0954 - val_acc: 0.9045 - val_f1_score: 0.8932 - val_f1_sample: 0.9133\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0469 - acc: 0.9400 - f1_score: 0.9326 - f1_sample: 0.9499\n",
      "Epoch 00014: val_f1_sample did not improve from 0.91563\n",
      "288/288 [==============================] - 132s 459ms/step - loss: 0.0469 - acc: 0.9400 - f1_score: 0.9326 - f1_sample: 0.9499 - val_loss: nan - val_acc: 0.8879 - val_f1_score: 0.8741 - val_f1_sample: 0.8857\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.9422 - f1_score: 0.9331 - f1_sample: 0.9516\n",
      "Epoch 00015: val_f1_sample did not improve from 0.91563\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0441 - acc: 0.9422 - f1_score: 0.9331 - f1_sample: 0.9516 - val_loss: 0.1087 - val_acc: 0.8958 - val_f1_score: 0.8800 - val_f1_sample: 0.9001\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0422 - acc: 0.9437 - f1_score: 0.9355 - f1_sample: 0.9550\n",
      "Epoch 00016: val_f1_sample improved from 0.91563 to 0.91942, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 132s 458ms/step - loss: 0.0422 - acc: 0.9437 - f1_score: 0.9355 - f1_sample: 0.9550 - val_loss: 0.0901 - val_acc: 0.9201 - val_f1_score: 0.9062 - val_f1_sample: 0.9194\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0381 - acc: 0.9459 - f1_score: 0.9395 - f1_sample: 0.9593\n",
      "Epoch 00017: val_f1_sample did not improve from 0.91942\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0381 - acc: 0.9459 - f1_score: 0.9395 - f1_sample: 0.9593 - val_loss: 0.1038 - val_acc: 0.9030 - val_f1_score: 0.8969 - val_f1_sample: 0.9115\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0384 - acc: 0.9472 - f1_score: 0.9392 - f1_sample: 0.9578\n",
      "Epoch 00018: val_f1_sample did not improve from 0.91942\n",
      "288/288 [==============================] - 132s 457ms/step - loss: 0.0384 - acc: 0.9472 - f1_score: 0.9392 - f1_sample: 0.9578 - val_loss: 0.0894 - val_acc: 0.9134 - val_f1_score: 0.8987 - val_f1_sample: 0.9078\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0352 - acc: 0.9508 - f1_score: 0.9420 - f1_sample: 0.9622\n",
      "Epoch 00019: val_f1_sample improved from 0.91942 to 0.92124, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 134s 467ms/step - loss: 0.0352 - acc: 0.9508 - f1_score: 0.9420 - f1_sample: 0.9622 - val_loss: 0.0861 - val_acc: 0.9196 - val_f1_score: 0.9061 - val_f1_sample: 0.9212\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0338 - acc: 0.9500 - f1_score: 0.9416 - f1_sample: 0.9637\n",
      "Epoch 00020: val_f1_sample did not improve from 0.92124\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0338 - acc: 0.9500 - f1_score: 0.9416 - f1_sample: 0.9637 - val_loss: 0.1043 - val_acc: 0.9112 - val_f1_score: 0.9015 - val_f1_sample: 0.9159\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0309 - acc: 0.9544 - f1_score: 0.9453 - f1_sample: 0.9667\n",
      "Epoch 00021: val_f1_sample did not improve from 0.92124\n",
      "288/288 [==============================] - 132s 457ms/step - loss: 0.0309 - acc: 0.9544 - f1_score: 0.9453 - f1_sample: 0.9667 - val_loss: 0.0985 - val_acc: 0.9105 - val_f1_score: 0.9011 - val_f1_sample: 0.9143\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0319 - acc: 0.9534 - f1_score: 0.9433 - f1_sample: 0.9654\n",
      "Epoch 00022: val_f1_sample did not improve from 0.92124\n",
      "288/288 [==============================] - 129s 448ms/step - loss: 0.0319 - acc: 0.9534 - f1_score: 0.9433 - f1_sample: 0.9654 - val_loss: 0.1020 - val_acc: 0.9177 - val_f1_score: 0.9028 - val_f1_sample: 0.9127\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0289 - acc: 0.9546 - f1_score: 0.9472 - f1_sample: 0.9702\n",
      "Epoch 00023: val_f1_sample did not improve from 0.92124\n",
      "288/288 [==============================] - 129s 447ms/step - loss: 0.0289 - acc: 0.9546 - f1_score: 0.9472 - f1_sample: 0.9702 - val_loss: nan - val_acc: 0.8817 - val_f1_score: 0.8676 - val_f1_sample: 0.8912\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0281 - acc: 0.9541 - f1_score: 0.9477 - f1_sample: 0.9700\n",
      "Epoch 00024: val_f1_sample did not improve from 0.92124\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0281 - acc: 0.9541 - f1_score: 0.9477 - f1_sample: 0.9700 - val_loss: 0.1080 - val_acc: 0.9224 - val_f1_score: 0.9079 - val_f1_sample: 0.9183\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0259 - acc: 0.9545 - f1_score: 0.9494 - f1_sample: 0.9724\n",
      "Epoch 00025: val_f1_sample did not improve from 0.92124\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "288/288 [==============================] - 132s 457ms/step - loss: 0.0259 - acc: 0.9545 - f1_score: 0.9494 - f1_sample: 0.9724 - val_loss: 0.2777 - val_acc: 0.9102 - val_f1_score: 0.8965 - val_f1_sample: 0.9010\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9611 - f1_score: 0.9555 - f1_sample: 0.9789\n",
      "Epoch 00026: val_f1_sample improved from 0.92124 to 0.92244, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 133s 461ms/step - loss: 0.0204 - acc: 0.9611 - f1_score: 0.9555 - f1_sample: 0.9789 - val_loss: 0.1059 - val_acc: 0.9244 - val_f1_score: 0.9149 - val_f1_sample: 0.9224\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0198 - acc: 0.9563 - f1_score: 0.9547 - f1_sample: 0.9792\n",
      "Epoch 00027: val_f1_sample improved from 0.92244 to 0.92424, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_3.h5\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0198 - acc: 0.9563 - f1_score: 0.9547 - f1_sample: 0.9792 - val_loss: 0.1012 - val_acc: 0.9204 - val_f1_score: 0.9097 - val_f1_sample: 0.9242\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9596 - f1_score: 0.9559 - f1_sample: 0.9802\n",
      "Epoch 00028: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0193 - acc: 0.9596 - f1_score: 0.9559 - f1_sample: 0.9802 - val_loss: 0.1142 - val_acc: 0.9229 - val_f1_score: 0.9110 - val_f1_sample: 0.9200\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0168 - acc: 0.9584 - f1_score: 0.9569 - f1_sample: 0.9833\n",
      "Epoch 00029: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0168 - acc: 0.9584 - f1_score: 0.9569 - f1_sample: 0.9833 - val_loss: 0.1026 - val_acc: 0.9239 - val_f1_score: 0.9128 - val_f1_sample: 0.9224\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9611 - f1_score: 0.9574 - f1_sample: 0.9824\n",
      "Epoch 00030: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0170 - acc: 0.9611 - f1_score: 0.9574 - f1_sample: 0.9824 - val_loss: 0.1121 - val_acc: 0.9288 - val_f1_score: 0.9137 - val_f1_sample: 0.9237\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.9643 - f1_score: 0.9593 - f1_sample: 0.9838\n",
      "Epoch 00031: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0158 - acc: 0.9643 - f1_score: 0.9593 - f1_sample: 0.9838 - val_loss: 0.1156 - val_acc: 0.9293 - val_f1_score: 0.9147 - val_f1_sample: 0.9239\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0176 - acc: 0.9607 - f1_score: 0.9578 - f1_sample: 0.9828\n",
      "Epoch 00032: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0176 - acc: 0.9607 - f1_score: 0.9578 - f1_sample: 0.9828 - val_loss: 0.1490 - val_acc: 0.9067 - val_f1_score: 0.8895 - val_f1_sample: 0.8915\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.9593 - f1_score: 0.9584 - f1_sample: 0.9846\n",
      "Epoch 00033: val_f1_sample did not improve from 0.92424\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "288/288 [==============================] - 131s 455ms/step - loss: 0.0152 - acc: 0.9593 - f1_score: 0.9584 - f1_sample: 0.9846 - val_loss: 0.1125 - val_acc: 0.9149 - val_f1_score: 0.9028 - val_f1_sample: 0.9141\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9632 - f1_score: 0.9632 - f1_sample: 0.9876\n",
      "Epoch 00034: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 127s 442ms/step - loss: 0.0121 - acc: 0.9632 - f1_score: 0.9632 - f1_sample: 0.9876 - val_loss: 0.1238 - val_acc: 0.9216 - val_f1_score: 0.9120 - val_f1_sample: 0.9227\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0118 - acc: 0.9638 - f1_score: 0.9626 - f1_sample: 0.9874Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_f1_sample did not improve from 0.92424\n",
      "288/288 [==============================] - 131s 453ms/step - loss: 0.0118 - acc: 0.9638 - f1_score: 0.9626 - f1_sample: 0.9874 - val_loss: 0.1152 - val_acc: 0.9201 - val_f1_score: 0.9097 - val_f1_sample: 0.9226\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/100\n",
      "  2/288 [..............................] - ETA: 7:31 - loss: 0.6750 - acc: 0.2589 - f1_score: 0.2730 - f1_sample: 0.3051WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.9130s vs `on_train_batch_end` time: 2.2440s). Check your callbacks.\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1458 - acc: 0.8415 - f1_score: 0.8294 - f1_sample: 0.8295\n",
      "Epoch 00001: val_f1_sample improved from -inf to 0.87745, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 133s 461ms/step - loss: 0.1458 - acc: 0.8415 - f1_score: 0.8294 - f1_sample: 0.8295 - val_loss: 0.1322 - val_acc: 0.8780 - val_f1_score: 0.8560 - val_f1_sample: 0.8774\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0979 - acc: 0.8969 - f1_score: 0.8844 - f1_sample: 0.8907\n",
      "Epoch 00002: val_f1_sample improved from 0.87745 to 0.89659, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 132s 459ms/step - loss: 0.0979 - acc: 0.8969 - f1_score: 0.8844 - f1_sample: 0.8907 - val_loss: 0.0993 - val_acc: 0.8963 - val_f1_score: 0.8848 - val_f1_sample: 0.8966\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0860 - acc: 0.9054 - f1_score: 0.8934 - f1_sample: 0.9024\n",
      "Epoch 00003: val_f1_sample improved from 0.89659 to 0.90944, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0860 - acc: 0.9054 - f1_score: 0.8934 - f1_sample: 0.9024 - val_loss: 0.0892 - val_acc: 0.9050 - val_f1_score: 0.9016 - val_f1_sample: 0.9094\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0785 - acc: 0.9126 - f1_score: 0.9014 - f1_sample: 0.9119\n",
      "Epoch 00004: val_f1_sample did not improve from 0.90944\n",
      "288/288 [==============================] - 131s 454ms/step - loss: 0.0785 - acc: 0.9126 - f1_score: 0.9014 - f1_sample: 0.9119 - val_loss: 0.0958 - val_acc: 0.9184 - val_f1_score: 0.8958 - val_f1_sample: 0.9030\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0756 - acc: 0.9142 - f1_score: 0.9028 - f1_sample: 0.9149\n",
      "Epoch 00005: val_f1_sample did not improve from 0.90944\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0756 - acc: 0.9142 - f1_score: 0.9028 - f1_sample: 0.9149 - val_loss: 0.0884 - val_acc: 0.8938 - val_f1_score: 0.8861 - val_f1_sample: 0.8966\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0706 - acc: 0.9222 - f1_score: 0.9103 - f1_sample: 0.9210\n",
      "Epoch 00006: val_f1_sample did not improve from 0.90944\n",
      "288/288 [==============================] - 131s 454ms/step - loss: 0.0706 - acc: 0.9222 - f1_score: 0.9103 - f1_sample: 0.9210 - val_loss: 0.0873 - val_acc: 0.9137 - val_f1_score: 0.8948 - val_f1_sample: 0.9051\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0647 - acc: 0.9251 - f1_score: 0.9147 - f1_sample: 0.9270\n",
      "Epoch 00007: val_f1_sample improved from 0.90944 to 0.92033, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0647 - acc: 0.9251 - f1_score: 0.9147 - f1_sample: 0.9270 - val_loss: 0.0764 - val_acc: 0.9194 - val_f1_score: 0.9074 - val_f1_sample: 0.9203\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0637 - acc: 0.9259 - f1_score: 0.9142 - f1_sample: 0.9275\n",
      "Epoch 00008: val_f1_sample improved from 0.92033 to 0.92216, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 130s 453ms/step - loss: 0.0637 - acc: 0.9259 - f1_score: 0.9142 - f1_sample: 0.9275 - val_loss: 0.0727 - val_acc: 0.9249 - val_f1_score: 0.9098 - val_f1_sample: 0.9222\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0585 - acc: 0.9311 - f1_score: 0.9213 - f1_sample: 0.9338\n",
      "Epoch 00009: val_f1_sample improved from 0.92216 to 0.92522, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 128s 445ms/step - loss: 0.0585 - acc: 0.9311 - f1_score: 0.9213 - f1_sample: 0.9338 - val_loss: 0.0741 - val_acc: 0.9313 - val_f1_score: 0.9163 - val_f1_sample: 0.9252\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0567 - acc: 0.9319 - f1_score: 0.9216 - f1_sample: 0.9366\n",
      "Epoch 00010: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0567 - acc: 0.9319 - f1_score: 0.9216 - f1_sample: 0.9366 - val_loss: 0.0918 - val_acc: 0.9117 - val_f1_score: 0.8992 - val_f1_sample: 0.9096\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0544 - acc: 0.9330 - f1_score: 0.9235 - f1_sample: 0.9396\n",
      "Epoch 00011: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 130s 452ms/step - loss: 0.0544 - acc: 0.9330 - f1_score: 0.9235 - f1_sample: 0.9396 - val_loss: 0.0813 - val_acc: 0.9271 - val_f1_score: 0.9085 - val_f1_sample: 0.9230\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0486 - acc: 0.9392 - f1_score: 0.9306 - f1_sample: 0.9463\n",
      "Epoch 00012: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 131s 455ms/step - loss: 0.0486 - acc: 0.9392 - f1_score: 0.9306 - f1_sample: 0.9463 - val_loss: 0.0896 - val_acc: 0.9224 - val_f1_score: 0.9051 - val_f1_sample: 0.9124\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0499 - acc: 0.9372 - f1_score: 0.9301 - f1_sample: 0.9456\n",
      "Epoch 00013: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 131s 456ms/step - loss: 0.0499 - acc: 0.9372 - f1_score: 0.9301 - f1_sample: 0.9456 - val_loss: 0.0771 - val_acc: 0.9283 - val_f1_score: 0.9119 - val_f1_sample: 0.9208\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.9385 - f1_score: 0.9301 - f1_sample: 0.9473\n",
      "Epoch 00014: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 127s 441ms/step - loss: 0.0459 - acc: 0.9385 - f1_score: 0.9301 - f1_sample: 0.9473 - val_loss: 0.0902 - val_acc: 0.9105 - val_f1_score: 0.8965 - val_f1_sample: 0.9144\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0423 - acc: 0.9431 - f1_score: 0.9351 - f1_sample: 0.9535\n",
      "Epoch 00015: val_f1_sample did not improve from 0.92522\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "288/288 [==============================] - 132s 458ms/step - loss: 0.0423 - acc: 0.9431 - f1_score: 0.9351 - f1_sample: 0.9535 - val_loss: 0.0841 - val_acc: 0.9258 - val_f1_score: 0.9098 - val_f1_sample: 0.9198\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0367 - acc: 0.9473 - f1_score: 0.9392 - f1_sample: 0.9600\n",
      "Epoch 00016: val_f1_sample did not improve from 0.92522\n",
      "288/288 [==============================] - 133s 461ms/step - loss: 0.0367 - acc: 0.9473 - f1_score: 0.9392 - f1_sample: 0.9600 - val_loss: 0.0930 - val_acc: 0.9315 - val_f1_score: 0.9121 - val_f1_sample: 0.9176\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0340 - acc: 0.9508 - f1_score: 0.9424 - f1_sample: 0.9632\n",
      "Epoch 00017: val_f1_sample improved from 0.92522 to 0.92903, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 130s 451ms/step - loss: 0.0340 - acc: 0.9508 - f1_score: 0.9424 - f1_sample: 0.9632 - val_loss: 0.0911 - val_acc: 0.9271 - val_f1_score: 0.9122 - val_f1_sample: 0.9290\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0300 - acc: 0.9564 - f1_score: 0.9462 - f1_sample: 0.9677\n",
      "Epoch 00018: val_f1_sample did not improve from 0.92903\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0300 - acc: 0.9564 - f1_score: 0.9462 - f1_sample: 0.9677 - val_loss: 0.0868 - val_acc: 0.9306 - val_f1_score: 0.9142 - val_f1_sample: 0.9285\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0282 - acc: 0.9552 - f1_score: 0.9475 - f1_sample: 0.9703\n",
      "Epoch 00019: val_f1_sample did not improve from 0.92903\n",
      "288/288 [==============================] - 129s 449ms/step - loss: 0.0282 - acc: 0.9552 - f1_score: 0.9475 - f1_sample: 0.9703 - val_loss: 0.0991 - val_acc: 0.9303 - val_f1_score: 0.9138 - val_f1_sample: 0.9220\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0287 - acc: 0.9528 - f1_score: 0.9466 - f1_sample: 0.9688\n",
      "Epoch 00020: val_f1_sample did not improve from 0.92903\n",
      "288/288 [==============================] - 129s 447ms/step - loss: 0.0287 - acc: 0.9528 - f1_score: 0.9466 - f1_sample: 0.9688 - val_loss: 0.1168 - val_acc: 0.9154 - val_f1_score: 0.8977 - val_f1_sample: 0.9111\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0261 - acc: 0.9547 - f1_score: 0.9486 - f1_sample: 0.9722\n",
      "Epoch 00021: val_f1_sample did not improve from 0.92903\n",
      "288/288 [==============================] - 128s 444ms/step - loss: 0.0261 - acc: 0.9547 - f1_score: 0.9486 - f1_sample: 0.9722 - val_loss: 0.0921 - val_acc: 0.9249 - val_f1_score: 0.9127 - val_f1_sample: 0.9260\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.9575 - f1_score: 0.9505 - f1_sample: 0.9747\n",
      "Epoch 00022: val_f1_sample did not improve from 0.92903\n",
      "288/288 [==============================] - 130s 450ms/step - loss: 0.0239 - acc: 0.9575 - f1_score: 0.9505 - f1_sample: 0.9747 - val_loss: 0.1219 - val_acc: 0.9174 - val_f1_score: 0.9085 - val_f1_sample: 0.9173\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.9575 - f1_score: 0.9515 - f1_sample: 0.9752\n",
      "Epoch 00023: val_f1_sample did not improve from 0.92903\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "288/288 [==============================] - 131s 455ms/step - loss: 0.0243 - acc: 0.9575 - f1_score: 0.9515 - f1_sample: 0.9752 - val_loss: 0.1048 - val_acc: 0.9296 - val_f1_score: 0.9157 - val_f1_sample: 0.9283\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9599 - f1_score: 0.9544 - f1_sample: 0.9788\n",
      "Epoch 00024: val_f1_sample improved from 0.92903 to 0.92909, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 137s 477ms/step - loss: 0.0199 - acc: 0.9599 - f1_score: 0.9544 - f1_sample: 0.9788 - val_loss: 0.0990 - val_acc: 0.9306 - val_f1_score: 0.9180 - val_f1_sample: 0.9291\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0168 - acc: 0.9629 - f1_score: 0.9578 - f1_sample: 0.9831\n",
      "Epoch 00025: val_f1_sample improved from 0.92909 to 0.93077, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 140s 486ms/step - loss: 0.0168 - acc: 0.9629 - f1_score: 0.9578 - f1_sample: 0.9831 - val_loss: 0.1024 - val_acc: 0.9320 - val_f1_score: 0.9194 - val_f1_sample: 0.9308\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0186 - acc: 0.9605 - f1_score: 0.9571 - f1_sample: 0.9811\n",
      "Epoch 00026: val_f1_sample did not improve from 0.93077\n",
      "288/288 [==============================] - 141s 489ms/step - loss: 0.0186 - acc: 0.9605 - f1_score: 0.9571 - f1_sample: 0.9811 - val_loss: 0.1067 - val_acc: 0.9231 - val_f1_score: 0.9103 - val_f1_sample: 0.9218\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0162 - acc: 0.9613 - f1_score: 0.9571 - f1_sample: 0.9822\n",
      "Epoch 00027: val_f1_sample did not improve from 0.93077\n",
      "288/288 [==============================] - 141s 489ms/step - loss: 0.0162 - acc: 0.9613 - f1_score: 0.9571 - f1_sample: 0.9822 - val_loss: 0.1083 - val_acc: 0.9236 - val_f1_score: 0.9147 - val_f1_sample: 0.9274\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0142 - acc: 0.9648 - f1_score: 0.9606 - f1_sample: 0.9846\n",
      "Epoch 00028: val_f1_sample did not improve from 0.93077\n",
      "288/288 [==============================] - 138s 481ms/step - loss: 0.0142 - acc: 0.9648 - f1_score: 0.9606 - f1_sample: 0.9846 - val_loss: 0.1187 - val_acc: 0.9189 - val_f1_score: 0.9120 - val_f1_sample: 0.9230\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0126 - acc: 0.9644 - f1_score: 0.9606 - f1_sample: 0.9868\n",
      "Epoch 00029: val_f1_sample did not improve from 0.93077\n",
      "288/288 [==============================] - 139s 481ms/step - loss: 0.0126 - acc: 0.9644 - f1_score: 0.9606 - f1_sample: 0.9868 - val_loss: 0.1190 - val_acc: 0.9313 - val_f1_score: 0.9196 - val_f1_sample: 0.9281\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0146 - acc: 0.9672 - f1_score: 0.9612 - f1_sample: 0.9852\n",
      "Epoch 00030: val_f1_sample did not improve from 0.93077\n",
      "288/288 [==============================] - 143s 497ms/step - loss: 0.0146 - acc: 0.9672 - f1_score: 0.9612 - f1_sample: 0.9852 - val_loss: 0.1219 - val_acc: 0.9353 - val_f1_score: 0.9183 - val_f1_sample: 0.9302\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0139 - acc: 0.9671 - f1_score: 0.9616 - f1_sample: 0.9858\n",
      "Epoch 00031: val_f1_sample did not improve from 0.93077\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002560000168159604.\n",
      "288/288 [==============================] - 145s 504ms/step - loss: 0.0139 - acc: 0.9671 - f1_score: 0.9616 - f1_sample: 0.9858 - val_loss: 0.1169 - val_acc: 0.9301 - val_f1_score: 0.9151 - val_f1_sample: 0.9267\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0116 - acc: 0.9695 - f1_score: 0.9632 - f1_sample: 0.9875\n",
      "Epoch 00032: val_f1_sample improved from 0.93077 to 0.93094, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 145s 502ms/step - loss: 0.0116 - acc: 0.9695 - f1_score: 0.9632 - f1_sample: 0.9875 - val_loss: 0.1131 - val_acc: 0.9273 - val_f1_score: 0.9190 - val_f1_sample: 0.9309\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9655 - f1_score: 0.9658 - f1_sample: 0.9900\n",
      "Epoch 00033: val_f1_sample did not improve from 0.93094\n",
      "288/288 [==============================] - 140s 488ms/step - loss: 0.0098 - acc: 0.9655 - f1_score: 0.9658 - f1_sample: 0.9900 - val_loss: 0.1210 - val_acc: 0.9271 - val_f1_score: 0.9184 - val_f1_sample: 0.9290\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0097 - acc: 0.9680 - f1_score: 0.9669 - f1_sample: 0.9901\n",
      "Epoch 00034: val_f1_sample improved from 0.93094 to 0.93171, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0097 - acc: 0.9680 - f1_score: 0.9669 - f1_sample: 0.9901 - val_loss: 0.1248 - val_acc: 0.9293 - val_f1_score: 0.9186 - val_f1_sample: 0.9317\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9703 - f1_score: 0.9657 - f1_sample: 0.9902\n",
      "Epoch 00035: val_f1_sample did not improve from 0.93171\n",
      "288/288 [==============================] - 140s 486ms/step - loss: 0.0095 - acc: 0.9703 - f1_score: 0.9657 - f1_sample: 0.9902 - val_loss: 0.1161 - val_acc: 0.9268 - val_f1_score: 0.9167 - val_f1_sample: 0.9284\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9681 - f1_score: 0.9672 - f1_sample: 0.9906\n",
      "Epoch 00036: val_f1_sample did not improve from 0.93171\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0094 - acc: 0.9681 - f1_score: 0.9672 - f1_sample: 0.9906 - val_loss: 0.1282 - val_acc: 0.9251 - val_f1_score: 0.9154 - val_f1_sample: 0.9279\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0093 - acc: 0.9685 - f1_score: 0.9679 - f1_sample: 0.9902\n",
      "Epoch 00037: val_f1_sample did not improve from 0.93171\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0093 - acc: 0.9685 - f1_score: 0.9679 - f1_sample: 0.9902 - val_loss: 0.1305 - val_acc: 0.9313 - val_f1_score: 0.9212 - val_f1_sample: 0.9272\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9705 - f1_score: 0.9689 - f1_sample: 0.9905\n",
      "Epoch 00038: val_f1_sample did not improve from 0.93171\n",
      "288/288 [==============================] - 144s 499ms/step - loss: 0.0094 - acc: 0.9705 - f1_score: 0.9689 - f1_sample: 0.9905 - val_loss: 0.1270 - val_acc: 0.9335 - val_f1_score: 0.9212 - val_f1_sample: 0.9317\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0093 - acc: 0.9718 - f1_score: 0.9676 - f1_sample: 0.9906\n",
      "Epoch 00039: val_f1_sample improved from 0.93171 to 0.93267, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_4.h5\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0093 - acc: 0.9718 - f1_score: 0.9676 - f1_sample: 0.9906 - val_loss: 0.1224 - val_acc: 0.9281 - val_f1_score: 0.9196 - val_f1_sample: 0.9327\n",
      "Epoch 40/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0087 - acc: 0.9722 - f1_score: 0.9679 - f1_sample: 0.9914\n",
      "Epoch 00040: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0087 - acc: 0.9722 - f1_score: 0.9679 - f1_sample: 0.9914 - val_loss: 0.1359 - val_acc: 0.9318 - val_f1_score: 0.9183 - val_f1_sample: 0.9279\n",
      "Epoch 41/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0087 - acc: 0.9711 - f1_score: 0.9682 - f1_sample: 0.9911\n",
      "Epoch 00041: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0087 - acc: 0.9711 - f1_score: 0.9682 - f1_sample: 0.9911 - val_loss: 0.1379 - val_acc: 0.9281 - val_f1_score: 0.9146 - val_f1_sample: 0.9241\n",
      "Epoch 42/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9712 - f1_score: 0.9682 - f1_sample: 0.9905\n",
      "Epoch 00042: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0094 - acc: 0.9712 - f1_score: 0.9682 - f1_sample: 0.9905 - val_loss: 0.1279 - val_acc: 0.9315 - val_f1_score: 0.9192 - val_f1_sample: 0.9298\n",
      "Epoch 43/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9763 - f1_score: 0.9725 - f1_sample: 0.9932\n",
      "Epoch 00043: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 141s 491ms/step - loss: 0.0067 - acc: 0.9763 - f1_score: 0.9725 - f1_sample: 0.9932 - val_loss: 0.1510 - val_acc: 0.9308 - val_f1_score: 0.9209 - val_f1_sample: 0.9260\n",
      "Epoch 44/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9738 - f1_score: 0.9716 - f1_sample: 0.9912\n",
      "Epoch 00044: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0086 - acc: 0.9738 - f1_score: 0.9716 - f1_sample: 0.9912 - val_loss: 0.1379 - val_acc: 0.9271 - val_f1_score: 0.9166 - val_f1_sample: 0.9275\n",
      "Epoch 45/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9733 - f1_score: 0.9727 - f1_sample: 0.9925\n",
      "Epoch 00045: val_f1_sample did not improve from 0.93267\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00020480002276599408.\n",
      "288/288 [==============================] - 141s 489ms/step - loss: 0.0069 - acc: 0.9733 - f1_score: 0.9727 - f1_sample: 0.9925 - val_loss: 0.1380 - val_acc: 0.9293 - val_f1_score: 0.9190 - val_f1_sample: 0.9278\n",
      "Epoch 46/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9730 - f1_score: 0.9711 - f1_sample: 0.9919\n",
      "Epoch 00046: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0080 - acc: 0.9730 - f1_score: 0.9711 - f1_sample: 0.9919 - val_loss: 0.1311 - val_acc: 0.9315 - val_f1_score: 0.9217 - val_f1_sample: 0.9318\n",
      "Epoch 47/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9723 - f1_score: 0.9736 - f1_sample: 0.9936Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00047: val_f1_sample did not improve from 0.93267\n",
      "288/288 [==============================] - 141s 489ms/step - loss: 0.0066 - acc: 0.9723 - f1_score: 0.9736 - f1_sample: 0.9936 - val_loss: 0.1377 - val_acc: 0.9311 - val_f1_score: 0.9192 - val_f1_sample: 0.9284\n",
      "Epoch 00047: early stopping\n",
      "Epoch 1/100\n",
      "  2/288 [..............................] - ETA: 7:11 - loss: 0.6785 - acc: 0.2232 - f1_score: 0.2529 - f1_sample: 0.2476WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4157s vs `on_train_batch_end` time: 2.6019s). Check your callbacks.\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1453 - acc: 0.8445 - f1_score: 0.8314 - f1_sample: 0.8310\n",
      "Epoch 00001: val_f1_sample improved from -inf to 0.85819, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 147s 510ms/step - loss: 0.1453 - acc: 0.8445 - f1_score: 0.8314 - f1_sample: 0.8310 - val_loss: 0.1368 - val_acc: 0.8442 - val_f1_score: 0.8327 - val_f1_sample: 0.8582\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0985 - acc: 0.8950 - f1_score: 0.8822 - f1_sample: 0.8913\n",
      "Epoch 00002: val_f1_sample improved from 0.85819 to 0.89580, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0985 - acc: 0.8950 - f1_score: 0.8822 - f1_sample: 0.8913 - val_loss: 0.0872 - val_acc: 0.8991 - val_f1_score: 0.8881 - val_f1_sample: 0.8958\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0865 - acc: 0.9040 - f1_score: 0.8900 - f1_sample: 0.9005\n",
      "Epoch 00003: val_f1_sample improved from 0.89580 to 0.90069, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 143s 495ms/step - loss: 0.0865 - acc: 0.9040 - f1_score: 0.8900 - f1_sample: 0.9005 - val_loss: 0.0862 - val_acc: 0.8963 - val_f1_score: 0.8741 - val_f1_sample: 0.9007\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0789 - acc: 0.9133 - f1_score: 0.9008 - f1_sample: 0.9112\n",
      "Epoch 00004: val_f1_sample improved from 0.90069 to 0.91315, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 143s 495ms/step - loss: 0.0789 - acc: 0.9133 - f1_score: 0.9008 - f1_sample: 0.9112 - val_loss: 0.0830 - val_acc: 0.9182 - val_f1_score: 0.9019 - val_f1_sample: 0.9132\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0759 - acc: 0.9169 - f1_score: 0.9038 - f1_sample: 0.9132\n",
      "Epoch 00005: val_f1_sample did not improve from 0.91315\n",
      "288/288 [==============================] - 142s 491ms/step - loss: 0.0759 - acc: 0.9169 - f1_score: 0.9038 - f1_sample: 0.9132 - val_loss: 0.0859 - val_acc: 0.9065 - val_f1_score: 0.8923 - val_f1_sample: 0.9043\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0715 - acc: 0.9203 - f1_score: 0.9076 - f1_sample: 0.9189\n",
      "Epoch 00006: val_f1_sample did not improve from 0.91315\n",
      "288/288 [==============================] - 140s 484ms/step - loss: 0.0715 - acc: 0.9203 - f1_score: 0.9076 - f1_sample: 0.9189 - val_loss: 0.0841 - val_acc: 0.9100 - val_f1_score: 0.8968 - val_f1_sample: 0.9077\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0679 - acc: 0.9227 - f1_score: 0.9106 - f1_sample: 0.9232\n",
      "Epoch 00007: val_f1_sample did not improve from 0.91315\n",
      "288/288 [==============================] - 143s 495ms/step - loss: 0.0679 - acc: 0.9227 - f1_score: 0.9106 - f1_sample: 0.9232 - val_loss: 0.0887 - val_acc: 0.9082 - val_f1_score: 0.8969 - val_f1_sample: 0.9131\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0637 - acc: 0.9267 - f1_score: 0.9161 - f1_sample: 0.9284\n",
      "Epoch 00008: val_f1_sample improved from 0.91315 to 0.92160, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 145s 504ms/step - loss: 0.0637 - acc: 0.9267 - f1_score: 0.9161 - f1_sample: 0.9284 - val_loss: 0.0695 - val_acc: 0.9196 - val_f1_score: 0.9074 - val_f1_sample: 0.9216\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0589 - acc: 0.9326 - f1_score: 0.9204 - f1_sample: 0.9340\n",
      "Epoch 00009: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 146s 506ms/step - loss: 0.0589 - acc: 0.9326 - f1_score: 0.9204 - f1_sample: 0.9340 - val_loss: 0.0842 - val_acc: 0.9075 - val_f1_score: 0.8990 - val_f1_sample: 0.9118\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0585 - acc: 0.9349 - f1_score: 0.9221 - f1_sample: 0.9350\n",
      "Epoch 00010: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 144s 501ms/step - loss: 0.0585 - acc: 0.9349 - f1_score: 0.9221 - f1_sample: 0.9350 - val_loss: 0.0926 - val_acc: 0.9144 - val_f1_score: 0.9027 - val_f1_sample: 0.9057\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9337 - f1_score: 0.9228 - f1_sample: 0.9388\n",
      "Epoch 00011: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 138s 479ms/step - loss: 0.0554 - acc: 0.9337 - f1_score: 0.9228 - f1_sample: 0.9388 - val_loss: 0.0818 - val_acc: 0.9102 - val_f1_score: 0.8970 - val_f1_sample: 0.9092\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0531 - acc: 0.9347 - f1_score: 0.9261 - f1_sample: 0.9412\n",
      "Epoch 00012: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 144s 501ms/step - loss: 0.0531 - acc: 0.9347 - f1_score: 0.9261 - f1_sample: 0.9412 - val_loss: 0.1056 - val_acc: 0.8924 - val_f1_score: 0.8867 - val_f1_sample: 0.8929\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0498 - acc: 0.9402 - f1_score: 0.9285 - f1_sample: 0.9440\n",
      "Epoch 00013: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 140s 488ms/step - loss: 0.0498 - acc: 0.9402 - f1_score: 0.9285 - f1_sample: 0.9440 - val_loss: 0.0749 - val_acc: 0.9157 - val_f1_score: 0.9106 - val_f1_sample: 0.9192\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.9417 - f1_score: 0.9307 - f1_sample: 0.9476\n",
      "Epoch 00014: val_f1_sample did not improve from 0.92160\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "288/288 [==============================] - 143s 498ms/step - loss: 0.0459 - acc: 0.9417 - f1_score: 0.9307 - f1_sample: 0.9476 - val_loss: 0.0863 - val_acc: 0.9055 - val_f1_score: 0.9025 - val_f1_sample: 0.9116\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0403 - acc: 0.9461 - f1_score: 0.9361 - f1_sample: 0.9552\n",
      "Epoch 00015: val_f1_sample did not improve from 0.92160\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0403 - acc: 0.9461 - f1_score: 0.9361 - f1_sample: 0.9552 - val_loss: 0.0797 - val_acc: 0.9132 - val_f1_score: 0.9018 - val_f1_sample: 0.9164\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0356 - acc: 0.9515 - f1_score: 0.9414 - f1_sample: 0.9607\n",
      "Epoch 00016: val_f1_sample improved from 0.92160 to 0.92298, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 144s 499ms/step - loss: 0.0356 - acc: 0.9515 - f1_score: 0.9414 - f1_sample: 0.9607 - val_loss: 0.0867 - val_acc: 0.9258 - val_f1_score: 0.9132 - val_f1_sample: 0.9230\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9544 - f1_score: 0.9422 - f1_sample: 0.9614\n",
      "Epoch 00017: val_f1_sample improved from 0.92298 to 0.92388, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 143s 498ms/step - loss: 0.0351 - acc: 0.9544 - f1_score: 0.9422 - f1_sample: 0.9614 - val_loss: 0.0819 - val_acc: 0.9281 - val_f1_score: 0.9141 - val_f1_sample: 0.9239\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0338 - acc: 0.9492 - f1_score: 0.9431 - f1_sample: 0.9630\n",
      "Epoch 00018: val_f1_sample did not improve from 0.92388\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0338 - acc: 0.9492 - f1_score: 0.9431 - f1_sample: 0.9630 - val_loss: 0.0887 - val_acc: 0.9144 - val_f1_score: 0.9057 - val_f1_sample: 0.9198\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0323 - acc: 0.9499 - f1_score: 0.9431 - f1_sample: 0.9661\n",
      "Epoch 00019: val_f1_sample did not improve from 0.92388\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0323 - acc: 0.9499 - f1_score: 0.9431 - f1_sample: 0.9661 - val_loss: 0.0911 - val_acc: 0.9194 - val_f1_score: 0.9105 - val_f1_sample: 0.9191\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0299 - acc: 0.9542 - f1_score: 0.9461 - f1_sample: 0.9677\n",
      "Epoch 00020: val_f1_sample did not improve from 0.92388\n",
      "288/288 [==============================] - 143s 498ms/step - loss: 0.0299 - acc: 0.9542 - f1_score: 0.9461 - f1_sample: 0.9677 - val_loss: 0.0921 - val_acc: 0.9239 - val_f1_score: 0.9120 - val_f1_sample: 0.9229\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9585 - f1_score: 0.9497 - f1_sample: 0.9731\n",
      "Epoch 00021: val_f1_sample did not improve from 0.92388\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0253 - acc: 0.9585 - f1_score: 0.9497 - f1_sample: 0.9731 - val_loss: 0.1142 - val_acc: 0.9169 - val_f1_score: 0.9076 - val_f1_sample: 0.9172\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9557 - f1_score: 0.9476 - f1_sample: 0.9708\n",
      "Epoch 00022: val_f1_sample did not improve from 0.92388\n",
      "288/288 [==============================] - 141s 488ms/step - loss: 0.0279 - acc: 0.9557 - f1_score: 0.9476 - f1_sample: 0.9708 - val_loss: 0.0937 - val_acc: 0.9281 - val_f1_score: 0.9125 - val_f1_sample: 0.9230\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0256 - acc: 0.9581 - f1_score: 0.9506 - f1_sample: 0.9730\n",
      "Epoch 00023: val_f1_sample did not improve from 0.92388\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "288/288 [==============================] - 141s 489ms/step - loss: 0.0256 - acc: 0.9581 - f1_score: 0.9506 - f1_sample: 0.9730 - val_loss: 0.0880 - val_acc: 0.9209 - val_f1_score: 0.9109 - val_f1_sample: 0.9225\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0190 - acc: 0.9631 - f1_score: 0.9537 - f1_sample: 0.9811\n",
      "Epoch 00024: val_f1_sample improved from 0.92388 to 0.92490, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 143s 497ms/step - loss: 0.0190 - acc: 0.9631 - f1_score: 0.9537 - f1_sample: 0.9811 - val_loss: 0.0948 - val_acc: 0.9229 - val_f1_score: 0.9110 - val_f1_sample: 0.9249\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9629 - f1_score: 0.9568 - f1_sample: 0.9814\n",
      "Epoch 00025: val_f1_sample improved from 0.92490 to 0.92651, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0169 - acc: 0.9629 - f1_score: 0.9568 - f1_sample: 0.9814 - val_loss: 0.1008 - val_acc: 0.9239 - val_f1_score: 0.9166 - val_f1_sample: 0.9265\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0177 - acc: 0.9629 - f1_score: 0.9558 - f1_sample: 0.9805\n",
      "Epoch 00026: val_f1_sample did not improve from 0.92651\n",
      "288/288 [==============================] - 140s 486ms/step - loss: 0.0177 - acc: 0.9629 - f1_score: 0.9558 - f1_sample: 0.9805 - val_loss: 0.1123 - val_acc: 0.9206 - val_f1_score: 0.9091 - val_f1_sample: 0.9212\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0180 - acc: 0.9628 - f1_score: 0.9571 - f1_sample: 0.9817\n",
      "Epoch 00027: val_f1_sample improved from 0.92651 to 0.92798, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 145s 502ms/step - loss: 0.0180 - acc: 0.9628 - f1_score: 0.9571 - f1_sample: 0.9817 - val_loss: 0.0957 - val_acc: 0.9268 - val_f1_score: 0.9132 - val_f1_sample: 0.9280\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0183 - acc: 0.9626 - f1_score: 0.9565 - f1_sample: 0.9807\n",
      "Epoch 00028: val_f1_sample did not improve from 0.92798\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0183 - acc: 0.9626 - f1_score: 0.9565 - f1_sample: 0.9807 - val_loss: 0.1048 - val_acc: 0.9296 - val_f1_score: 0.9144 - val_f1_sample: 0.9241\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9623 - f1_score: 0.9570 - f1_sample: 0.9819\n",
      "Epoch 00029: val_f1_sample did not improve from 0.92798\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0170 - acc: 0.9623 - f1_score: 0.9570 - f1_sample: 0.9819 - val_loss: 0.0980 - val_acc: 0.9251 - val_f1_score: 0.9186 - val_f1_sample: 0.9271\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0144 - acc: 0.9676 - f1_score: 0.9591 - f1_sample: 0.9851\n",
      "Epoch 00030: val_f1_sample did not improve from 0.92798\n",
      "288/288 [==============================] - 139s 483ms/step - loss: 0.0144 - acc: 0.9676 - f1_score: 0.9591 - f1_sample: 0.9851 - val_loss: 0.1039 - val_acc: 0.9263 - val_f1_score: 0.9146 - val_f1_sample: 0.9250\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0139 - acc: 0.9665 - f1_score: 0.9609 - f1_sample: 0.9852\n",
      "Epoch 00031: val_f1_sample did not improve from 0.92798\n",
      "288/288 [==============================] - 143s 497ms/step - loss: 0.0139 - acc: 0.9665 - f1_score: 0.9609 - f1_sample: 0.9852 - val_loss: 0.1100 - val_acc: 0.9216 - val_f1_score: 0.9125 - val_f1_sample: 0.9239\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9668 - f1_score: 0.9610 - f1_sample: 0.9858\n",
      "Epoch 00032: val_f1_sample did not improve from 0.92798\n",
      "288/288 [==============================] - 141s 491ms/step - loss: 0.0141 - acc: 0.9668 - f1_score: 0.9610 - f1_sample: 0.9858 - val_loss: 0.1104 - val_acc: 0.9273 - val_f1_score: 0.9159 - val_f1_sample: 0.9232\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0130 - acc: 0.9674 - f1_score: 0.9631 - f1_sample: 0.9867\n",
      "Epoch 00033: val_f1_sample did not improve from 0.92798\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0002560000168159604.\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0130 - acc: 0.9674 - f1_score: 0.9631 - f1_sample: 0.9867 - val_loss: 0.1095 - val_acc: 0.9249 - val_f1_score: 0.9153 - val_f1_sample: 0.9273\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9680 - f1_score: 0.9638 - f1_sample: 0.9877\n",
      "Epoch 00034: val_f1_sample improved from 0.92798 to 0.92800, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0120 - acc: 0.9680 - f1_score: 0.9638 - f1_sample: 0.9877 - val_loss: 0.1038 - val_acc: 0.9311 - val_f1_score: 0.9178 - val_f1_sample: 0.9280\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9704 - f1_score: 0.9667 - f1_sample: 0.9896\n",
      "Epoch 00035: val_f1_sample did not improve from 0.92800\n",
      "288/288 [==============================] - 140s 486ms/step - loss: 0.0098 - acc: 0.9704 - f1_score: 0.9667 - f1_sample: 0.9896 - val_loss: 0.1132 - val_acc: 0.9283 - val_f1_score: 0.9180 - val_f1_sample: 0.9255\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9678 - f1_score: 0.9644 - f1_sample: 0.9884\n",
      "Epoch 00036: val_f1_sample did not improve from 0.92800\n",
      "288/288 [==============================] - 137s 476ms/step - loss: 0.0102 - acc: 0.9678 - f1_score: 0.9644 - f1_sample: 0.9884 - val_loss: 0.1163 - val_acc: 0.9256 - val_f1_score: 0.9141 - val_f1_sample: 0.9220\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0105 - acc: 0.9710 - f1_score: 0.9676 - f1_sample: 0.9891\n",
      "Epoch 00037: val_f1_sample did not improve from 0.92800\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0105 - acc: 0.9710 - f1_score: 0.9676 - f1_sample: 0.9891 - val_loss: 0.1198 - val_acc: 0.9278 - val_f1_score: 0.9174 - val_f1_sample: 0.9262\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0103 - acc: 0.9711 - f1_score: 0.9662 - f1_sample: 0.9893\n",
      "Epoch 00038: val_f1_sample improved from 0.92800 to 0.92801, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0103 - acc: 0.9711 - f1_score: 0.9662 - f1_sample: 0.9893 - val_loss: 0.1230 - val_acc: 0.9256 - val_f1_score: 0.9181 - val_f1_sample: 0.9280\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0093 - acc: 0.9695 - f1_score: 0.9678 - f1_sample: 0.9903\n",
      "Epoch 00039: val_f1_sample improved from 0.92801 to 0.93102, saving model to /app/_data/models/final/f1_sample/eff4_0891_f1_5.h5\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0093 - acc: 0.9695 - f1_score: 0.9678 - f1_sample: 0.9903 - val_loss: 0.1212 - val_acc: 0.9330 - val_f1_score: 0.9218 - val_f1_sample: 0.9310\n",
      "Epoch 40/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9689 - f1_score: 0.9674 - f1_sample: 0.9899\n",
      "Epoch 00040: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 140s 485ms/step - loss: 0.0095 - acc: 0.9689 - f1_score: 0.9674 - f1_sample: 0.9899 - val_loss: 0.1201 - val_acc: 0.9301 - val_f1_score: 0.9194 - val_f1_sample: 0.9277\n",
      "Epoch 41/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9740 - f1_score: 0.9681 - f1_sample: 0.9897\n",
      "Epoch 00041: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0095 - acc: 0.9740 - f1_score: 0.9681 - f1_sample: 0.9897 - val_loss: 0.1304 - val_acc: 0.9239 - val_f1_score: 0.9122 - val_f1_sample: 0.9228\n",
      "Epoch 42/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9739 - f1_score: 0.9718 - f1_sample: 0.9909\n",
      "Epoch 00042: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 140s 485ms/step - loss: 0.0083 - acc: 0.9739 - f1_score: 0.9718 - f1_sample: 0.9909 - val_loss: 0.1230 - val_acc: 0.9281 - val_f1_score: 0.9148 - val_f1_sample: 0.9256\n",
      "Epoch 43/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9697 - f1_score: 0.9696 - f1_sample: 0.9910\n",
      "Epoch 00043: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 143s 498ms/step - loss: 0.0088 - acc: 0.9697 - f1_score: 0.9696 - f1_sample: 0.9910 - val_loss: 0.1235 - val_acc: 0.9281 - val_f1_score: 0.9183 - val_f1_sample: 0.9283\n",
      "Epoch 44/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9690 - f1_score: 0.9697 - f1_sample: 0.9911\n",
      "Epoch 00044: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 141s 490ms/step - loss: 0.0086 - acc: 0.9690 - f1_score: 0.9697 - f1_sample: 0.9911 - val_loss: 0.1242 - val_acc: 0.9268 - val_f1_score: 0.9180 - val_f1_sample: 0.9251\n",
      "Epoch 45/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9732 - f1_score: 0.9715 - f1_sample: 0.9914\n",
      "Epoch 00045: val_f1_sample did not improve from 0.93102\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00020480002276599408.\n",
      "288/288 [==============================] - 143s 496ms/step - loss: 0.0080 - acc: 0.9732 - f1_score: 0.9715 - f1_sample: 0.9914 - val_loss: 0.1229 - val_acc: 0.9239 - val_f1_score: 0.9145 - val_f1_sample: 0.9248\n",
      "Epoch 46/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9741 - f1_score: 0.9703 - f1_sample: 0.9925\n",
      "Epoch 00046: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 142s 492ms/step - loss: 0.0076 - acc: 0.9741 - f1_score: 0.9703 - f1_sample: 0.9925 - val_loss: 0.1235 - val_acc: 0.9291 - val_f1_score: 0.9193 - val_f1_sample: 0.9283\n",
      "Epoch 47/100\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9729 - f1_score: 0.9734 - f1_sample: 0.9930Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00047: val_f1_sample did not improve from 0.93102\n",
      "288/288 [==============================] - 142s 494ms/step - loss: 0.0069 - acc: 0.9729 - f1_score: 0.9734 - f1_sample: 0.9930 - val_loss: 0.1184 - val_acc: 0.9221 - val_f1_score: 0.9153 - val_f1_sample: 0.9258\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(\n",
    "    skf.split(df_labels[\"image\"], df_labels[\"labels\"])\n",
    "):\n",
    "    train, valid = df_labels.loc[train_index], df_labels.loc[valid_index]\n",
    "    model_name = \"eff4_0891_f1_\" + str(i + 1) + \".h5\"\n",
    "    log_dir = 'logs_891_f1_'+str(i + 1)+'/'\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_f1_sample\",\n",
    "            patience=8,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"/app/_data/models/final/f1_sample/\" + model_name,\n",
    "            monitor=\"val_f1_sample\",\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode=\"max\",\n",
    "            save_freq=\"epoch\",\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_f1_sample\",\n",
    "            factor=0.8,\n",
    "            patience=6,\n",
    "            verbose=1,\n",
    "            mode=\"max\",\n",
    "            min_delta=1e-4,\n",
    "            min_lr=0.00000001,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=\"/app/.tensorboard/\"+log_dir, histogram_freq=0\n",
    "        ),\n",
    "        keras.callbacks.experimental.BackupAndRestore(\n",
    "    '/app/_data/models/final/f1_sample/backup/'\n",
    ")\n",
    "    ]\n",
    "\n",
    "    gen_train = Generator(\n",
    "        df=train,\n",
    "        images_src_dir=TRAIN_IMG_PATH,\n",
    "        target_image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        augment=True,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "    )\n",
    "    gen_valid = Generator(\n",
    "        df=valid,\n",
    "        images_src_dir=TRAIN_IMG_PATH,\n",
    "        target_image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        augment=False,\n",
    "        crop=False,\n",
    "        resize=False,\n",
    "    )\n",
    "    model = get_model()\n",
    "    history = model.fit(\n",
    "        gen_train,\n",
    "        validation_data=gen_valid,\n",
    "        epochs=100,\n",
    "        steps_per_epoch=train.shape[0]//BATCH_SIZE,\n",
    "        validation_steps=valid.shape[0]//BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        workers = 10,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(file_path):\n",
    "    img = tf.io.read_file(TRAIN_IMG_PATH + file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def predict_new(path, model):\n",
    "    img = parse_all(path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    pred = model.predict(img)\n",
    "    return pred_to_labels(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  image                   labels\n",
      "0  bfc6d90f402f4c34.jpg  frog_eye_leaf_spot scab\n",
      "1  9eb93fe282326266.jpg           powdery_mildew\n",
      "2  f4cb3a8f41b413e4.jpg       frog_eye_leaf_spot\n",
      "3  98322eab16bef2c1.jpg                     rust\n",
      "4  dad5d6250cae80b7.jpg                  complex\n"
     ]
    }
   ],
   "source": [
    "df_sub = pd.DataFrame(columns=[\"image\", \"labels\"])\n",
    "for img_name in os.listdir(TRAIN_IMG_PATH):\n",
    "    pred = predict_new(img_name, model)\n",
    "\n",
    "    df_sub = df_sub.append({\"image\": img_name, \"labels\": pred}, ignore_index=True)\n",
    "\n",
    "print(df_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.merge(\n",
    "    labels_21_20[[\"image\", \"labels\"]],\n",
    "    on=\"image\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_pred\", \"_true\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"/app/sandbox/wrong_predictions/eff4/eff4_ns_cropped_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complex                    22\n",
       "scab                       10\n",
       "rust                        7\n",
       "frog_eye_leaf_spot          5\n",
       "scab frog_eye_leaf_spot     1\n",
       "rust frog_eye_leaf_spot     1\n",
       "powdery_mildew complex      1\n",
       "powdery_mildew              1\n",
       "healthy                     1\n",
       "Name: labels_true, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub[\"labels_pred\"] == \"\"][\"labels_true\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab frog_eye_leaf_spot            682\n",
       "complex                            438\n",
       "scab frog_eye_leaf_spot complex    200\n",
       "frog_eye_leaf_spot complex         165\n",
       "scab                               124\n",
       "rust frog_eye_leaf_spot            118\n",
       "rust complex                        91\n",
       "powdery_mildew complex              87\n",
       "rust                                74\n",
       "frog_eye_leaf_spot                  71\n",
       "healthy                             19\n",
       "powdery_mildew                       7\n",
       "Name: labels_true, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub[\"labels_pred\"] != df_sub[\"labels_true\"]][\"labels_true\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
