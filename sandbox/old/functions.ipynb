{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resizing and saving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize_im(\n",
    "#     image_name, new_folder_name=\"small_nearest\", size=(224, 224), resample=Image.NEAREST\n",
    "# ):\n",
    "#     image = Image.open(PATH + \"train_images/\" + image_name)\n",
    "#     image = image.resize(size, resample=resample)\n",
    "#     if not os.path.isdir(PATH + new_folder_name + \"/\"):\n",
    "#         os.mkdir(PATH + new_folder_name + \"/\")\n",
    "#     image.save(PATH  + new_folder_name + \"/\" + image_name)\n",
    "# labels['image'].apply(resize_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights for multilabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[\"multi_label\"] = labels[\"labels\"].astype(\"category\").cat.codes\n",
    "# dict_weights = (\n",
    "#     1\n",
    "#     / labels[\"multi_label\"].value_counts()\n",
    "#     / np.sum(1 / labels[\"multi_label\"].value_counts())\n",
    "# ).to_dict()\n",
    "# def weight_fill(x):\n",
    "#     return dict_weights[x]\n",
    "# labels[\"weights\"] = labels[\"multi_label\"].apply(weight_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyImageDataGenerator(ImageDataGenerator):\n",
    "#     def __init__(self, std_normalization=True):\n",
    "#         self.std_normalization = std_normalization\n",
    "\n",
    "#     if self.std_normalization:\n",
    "#         x = x.astype(np.float32)\n",
    "#         x = x - np.mean(x, axis=(0, 1))\n",
    "#         x = x / np.std(x, axis=(0, 1))\n",
    "\n",
    "#     else:\n",
    "#         warnings.warn(\"Ничего не получилось\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image, ImageStat\n",
    "\n",
    "# image_folder = os.path.join(PATH, \"small_nearest\")\n",
    "# image_files = labels['image'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean pix for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_files_3 = pd.DataFrame()\n",
    "# for file in tqdm(image_files):\n",
    "#     image = Image.open(os.path.join(image_folder, file))\n",
    "#     pix_mean = ImageStat.Stat(image).mean\n",
    "#     duplicate_files_3.loc[file, 'pix_mean1'] = pix_mean[0]\n",
    "#     duplicate_files_3.loc[file, 'pix_mean2'] = pix_mean[1]\n",
    "#     duplicate_files_3.loc[file, 'pix_mean3'] = pix_mean[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_files = duplicate_files.reset_index(drop=False)\n",
    "# duplicate_files.columns = ['image', 'pix_mean1', 'pix_mean2', 'pix_mean3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.round or without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_array = np.round(duplicate_files[['pix_mean1', 'pix_mean2', 'pix_mean3']].values)\n",
    "# duplicates_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_duplicates = []\n",
    "# for i in tqdm(range(len(duplicates_array))):\n",
    "#     for j in range(len(duplicates_array)):\n",
    "#         val1 = duplicates_array[i]\n",
    "#         val2 = duplicates_array[j]\n",
    "#         if i != j:\n",
    "#             if all(val1 == val2):\n",
    "#                 list_duplicates.append([duplicate_files.loc[i, 'image'], duplicate_files.loc[j, 'image']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicates = pd.DataFrame(list_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for im1, im2 in df_duplicates.values:\n",
    "#     plt.figure(figsize=(10,3))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.title(labels[labels['image'] == im1]['labels'].tolist())\n",
    "#     plt.imshow(Image.open(image_folder+'/'+im1))\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.title(labels[labels['image'] == im2]['labels'].tolist())\n",
    "#     plt.imshow(Image.open(image_folder+'/'+im2))\n",
    "#     plt.show();\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicates via hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imagehash\n",
    "# funcs = [\n",
    "#         imagehash.average_hash,\n",
    "#         imagehash.phash,\n",
    "#         imagehash.dhash,\n",
    "#         imagehash.whash,\n",
    "#     ]\n",
    "\n",
    "# image_ids = []\n",
    "# hashes = []\n",
    "\n",
    "# for img_id in tqdm(labels['image']):\n",
    "#     image = Image.open(image_folder+'/'+img_id)\n",
    "#     image_ids.append(img_id)\n",
    "#     hashes.append(np.array([f(image).hash for f in funcs]).reshape(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashes_all = np.array(hashes)\n",
    "# hashes_all = torch.Tensor(hashes_all.astype(int))\n",
    "\n",
    "# %time sims = np.array([(hashes_all[i] == hashes_all).sum(dim=1).numpy()/256 for i in range(hashes_all.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices1 = np.where(sims > 0.9)\n",
    "# indices2 = np.where(indices1[0] != indices1[1])\n",
    "# image_ids1 = [image_ids[i] for i in indices1[0][indices2]]\n",
    "# image_ids2 = [image_ids[i] for i in indices1[1][indices2]]\n",
    "\n",
    "# dups = [list(sorted([image_ids1,image_ids2])) for image_ids1, image_ids2 in zip(image_ids1, image_ids2)]\n",
    "# print('found %d duplicates' % len(dups))\n",
    "# for row in dups:\n",
    "#     print(','.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_test(threshold = True):\n",
    "#     sample_submission_copy = sample_submission.copy()\n",
    "\n",
    "#     if threshold:\n",
    "#         prediction_valid_results = model.predict(valid)\n",
    "#         prediction_valid = pd.DataFrame(prediction_valid_results, columns=feature_columns).join(pd.DataFrame(valid._targets, columns=feature_columns), rsuffix='_t')\n",
    "#         best_thresholds = pd.DataFrame()\n",
    "#         for col in feature_columns:\n",
    "#             f1_init = 0\n",
    "#             for th in np.linspace(0.01, 1, 500):\n",
    "#                 f1 = metrics.f1_score(prediction_valid[col +'_t'], prediction_valid[col]>th)\n",
    "#                 if f1>f1_init:\n",
    "#                     f1_init = f1\n",
    "#                     best_thresholds.loc['threshold', col] = th\n",
    "#                     best_thresholds.loc['f1', col] = f1\n",
    "#         test_prediction = pd.DataFrame(model.predict(test), columns=feature_columns, index = sample_submission.index)\n",
    "#         for col in feature_columns:\n",
    "#             sample_submission_copy[col] = test_prediction[col]>best_thresholds.loc['threshold', col]\n",
    "#             sample_submission_copy[col] = sample_submission_copy[col].replace({True: col, False: \"\"})\n",
    "#         sample_submission_copy['labels'] = sample_submission_copy[feature_columns].apply(\" \".join, axis=1).str.split().str.join(sep=\" \")\n",
    "#         return sample_submission_copy\n",
    "# #     , best_thresholds, prediction_valid\n",
    "#     else:\n",
    "#         test_prediction = pd.DataFrame(model.predict(test), columns=feature_columns, index = sample_submission.index)\n",
    "#         for col in feature_columns:\n",
    "#             sample_submission_copy[col] = test_prediction[col]>0.5\n",
    "#             sample_submission_copy[col] = sample_submission_copy[col].replace({True: col, False: \"\"})\n",
    "#         sample_submission_copy['labels'] = sample_submission_copy[feature_columns].apply(\" \".join, axis=1).str.split().str.join(sep=\" \")\n",
    "#         return sample_submission_copy\n",
    "\n",
    "# sample_submission_copy = predict_test(threshold=True)\n",
    "# sample_submission_copy[['image', 'labels']].to_csv(\"submission.csv\",index=False)\n",
    "# sample_submission_copy[['image', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr = model.predict(test)\n",
    "# sample_submission_copy = sample_submission.copy()\n",
    "# for i in range(len(pr)):\n",
    "#     sample_submission_copy.loc[i,'labels'] = ' '.join(list(map(lambda x,y: x*y, [pr>0.5][0][i],feature_columns)))\n",
    "# sample_submission_copy.to_csv(\"submission.csv\",index=False)\n",
    "# sample_submission_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, valid_df = train_test_split(\n",
    "#     labels, train_size=0.8, random_state=SEED, stratify=labels[\"labels\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     print(col, train_df[train_df[col] == 1][col].sum())\n",
    "\n",
    "\n",
    "\n",
    "# powdery_mildew = (\n",
    "#     train_df[train_df[\"powdery_mildew\"] == 1]\n",
    "#     .sample(\n",
    "#         n=(3000 - train_df[train_df[\"powdery_mildew\"] == 1].shape[0]),\n",
    "#         replace=True,\n",
    "#         random_state=SEED,\n",
    "#     )\n",
    "# )\n",
    "# rust = (\n",
    "#     train_df[train_df[\"rust\"] == 1]\n",
    "#     .sample(\n",
    "#         n=(3000 - train_df[train_df[\"rust\"] == 1].shape[0]),\n",
    "#         replace=True,\n",
    "#         random_state=SEED,\n",
    "#     )\n",
    "# )\n",
    "# complex_df = (\n",
    "#     train_df[train_df[\"complex\"] == 1]\n",
    "#     .sample(\n",
    "#         n=(3000 - train_df[train_df[\"complex\"] == 1].shape[0]),\n",
    "#         replace=True,\n",
    "#         random_state=SEED,\n",
    "#     )\n",
    "# )\n",
    "# train_df_new = pd.concat(\n",
    "#     [\n",
    "#         train_df,\n",
    "#         powdery_mildew,\n",
    "#         rust,\n",
    "#         complex_df,\n",
    "#     ],axis=0,ignore_index=True, \n",
    "# )\n",
    "\n",
    "\n",
    "# train_df_new.shape\n",
    "# train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(predict_train =False, predict_valid=False, all_img = False):\n",
    "    if all_img:\n",
    "        all_img = ImageDataGenerator(rescale=1.0 / 255).flow_from_dataframe(\n",
    "        dataframe=labels,\n",
    "        directory=PATH + \"train_images/small_bicubic/\",\n",
    "        x_col=\"image\",\n",
    "        y_col=feature_columns,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=\"raw\",\n",
    "        seed=SEED,\n",
    "        interpolation = 'bicubic',\n",
    "        shuffle=False,\n",
    "    )\n",
    "        prediction_all = model.predict(all_img)\n",
    "        prediction = pd.DataFrame(prediction_all, columns=feature_columns).join(pd.DataFrame(all_img._targets, columns=feature_columns), rsuffix='_true', lsuffix = '_pred')\n",
    "        prediction.index = all_img.filenames\n",
    "    elif predict_valid:       \n",
    "        prediction_valid = model.predict(valid)\n",
    "        prediction = pd.DataFrame(prediction_valid, columns=feature_columns).join(pd.DataFrame(valid._targets, columns=feature_columns), rsuffix='_true', lsuffix = '_pred')\n",
    "        prediction.index = valid.filenames\n",
    "    elif predict_train:\n",
    "        train = train_datagen.flow_from_dataframe(\n",
    "            dataframe=labels,\n",
    "            directory=PATH + \"train_images/small_bicubic/\",\n",
    "            x_col=\"image\",\n",
    "            y_col=feature_columns,\n",
    "            target_size=IMAGE_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode=\"raw\",\n",
    "            subset=\"training\",\n",
    "            seed=SEED,\n",
    "            interpolation = 'bicubic',\n",
    "            shuffle=False\n",
    "        )\n",
    "        prediction_train = model.predict(train)\n",
    "        prediction = pd.DataFrame(prediction_train, columns=feature_columns).join(pd.DataFrame(train._targets, columns=feature_columns), rsuffix='_true', lsuffix = '_pred')\n",
    "        prediction.index = train.filenames\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(prediction_df, show=True):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    for col in feature_columns:\n",
    "        pres = metrics.precision_score(prediction_df[col+'_true'], prediction_df[col+'_pred']>0.5)\n",
    "        rec = metrics.recall_score(prediction_df[col+'_true'], prediction_df[col+'_pred']>0.5)\n",
    "        f1 = metrics.f1_score(prediction_df[col+'_true'], prediction_df[col+'_pred']>0.5)\n",
    "        metrics_df.loc['precision', col] = pres\n",
    "        metrics_df.loc['recall', col] = rec\n",
    "        metrics_df.loc['f1_score', col] = f1\n",
    "    if show:\n",
    "        print(metrics_df)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_prediction(prediction_df, treshold = 0.5):\n",
    "    list_wrong_prediction = []\n",
    "    prediction = prediction_df.copy()\n",
    "    for col in feature_columns:\n",
    "        prediction[col+'_pred'] = prediction[col+'_pred'] > treshold\n",
    "        prediction[col+'_pred'] = prediction[col+'_pred'].replace({True: 1, False: 0})\n",
    "        ids = prediction[prediction[col+'_true'] != prediction[col+'_pred']].index.tolist()\n",
    "        list_wrong_prediction.extend(ids)\n",
    "    list_wrong_prediction = set(list_wrong_prediction)\n",
    "    wrong_prediction = prediction.loc[list_wrong_prediction]\n",
    "    for img in wrong_prediction.index.tolist():\n",
    "        pred = wrong_prediction.loc[img][:6].values\n",
    "        real = wrong_prediction.loc[img][6:12].values\n",
    "        pred_names = ' '.join(list(map(lambda x,y: x*y, pred,feature_columns)))\n",
    "        pred_names = ' '.join(pred_names.split())\n",
    "        real_names = ' '.join(list(map(lambda x,y: x*y, real,feature_columns)))\n",
    "        real_names = ' '.join(real_names.split())\n",
    "        wrong_prediction.loc[img, 'pred_labels'] = pred_names\n",
    "        wrong_prediction.loc[img, 'real_labels'] = real_names\n",
    "    return prediction, wrong_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrongs(df, num_im = 20):\n",
    "    plt.figure(figsize=(15,(num_im//4)*5))\n",
    "    for i, img in enumerate(df.sample(num_im).index.tolist()):\n",
    "        image = Image.open('/kaggle/input/plant-small/small_nearest/'+img)\n",
    "        plt.subplot(num_im//4, 4, i+1)\n",
    "        plt.title('predicted: '+df.loc[img,'pred_labels']+ '\\nreal: '+df.loc[img,'real_labels'])\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
